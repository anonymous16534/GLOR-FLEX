{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a6526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "# from bboxes.dtbb import DecisionTreeBlackBox\n",
    "# from bboxes.nnbb import NeuralNetworkBlackBox\n",
    "# from bboxes.rfbb import RandomForestBlackBox\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from new_trepan.trepan_generation import TrePanGenerator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from abstract import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a44eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_1, dropout, output, input):\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, output)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_1, hidden_2, dropout, output,\n",
    "                 input\n",
    "                 ):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_2, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Layer3Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_1, hidden_2, hidden_3,\n",
    "                 dropout, output, input\n",
    "                 ):\n",
    "\n",
    "        super(Layer3Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_3, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_simple_net(hidden_1, dropout, output, input):\n",
    "    net = SimpleNet(hidden_1, dropout, output, input)\n",
    "    return net\n",
    "\n",
    "\n",
    "def create_net(hidden_1=300, hidden_2=100, dropout=0.1, output=2,\n",
    "               input=85):\n",
    "    net = Net(hidden_1, hidden_2, dropout, output,\n",
    "              input)\n",
    "    return net\n",
    "\n",
    "def create_layer3net(hidden_layer_1, hidden_layer_2, hidden_layer_3,\n",
    "                     dropout_rate, n_classes, n_features):\n",
    "    net = Layer3Net(hidden_layer_1, hidden_layer_2, hidden_layer_3,\n",
    "                    dropout_rate, n_classes, n_features)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fea949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_1, dropout, output, input):\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, output)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_1, hidden_2, dropout, output,\n",
    "                 input\n",
    "                 ):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_2, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Layer3Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_1, hidden_2, hidden_3,\n",
    "                 dropout, output, input\n",
    "                 ):\n",
    "\n",
    "        super(Layer3Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_3, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_simple_net(hidden_1, dropout, output, input):\n",
    "    net = SimpleNet(hidden_1, dropout, output, input)\n",
    "    return net\n",
    "\n",
    "\n",
    "def create_net(hidden_1=300, hidden_2=100, dropout=0.1, output=2,\n",
    "               input=85):\n",
    "    net = Net(hidden_1, hidden_2, dropout, output,\n",
    "              input)\n",
    "    return net\n",
    "\n",
    "def create_layer3net(hidden_layer_1, hidden_layer_2, hidden_layer_3,\n",
    "                     dropout_rate, n_classes, n_features):\n",
    "    net = Layer3Net(hidden_layer_1, hidden_layer_2, hidden_layer_3,\n",
    "                    dropout_rate, n_classes, n_features)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e6ca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=39, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb0 = create_net(input = 39, hidden_1  = 256, hidden_2 = 256, dropout = 0.1, output = 2  )\n",
    "ckpt_0 = torch.load(open('path of the model', 'rb'))\n",
    "value_0 = ckpt_0.model\n",
    "bb0.load_state_dict(value_0)\n",
    "bb0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9d7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score(accuracy, depth, weight_accuracy=1.0, weight_depth=1.0):\n",
    "    score = (weight_accuracy * accuracy) + (weight_depth * (1 / depth))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788bf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score_sorting(config_list, gs_scores):\n",
    "    scores = []\n",
    "    for i, conf in enumerate(config_list):\n",
    "        depth = conf['max_depth']\n",
    "        sr = gs_scores[i]\n",
    "        new_sr = new_score(sr, depth, weight_accuracy=0.2, weight_depth=0.1)\n",
    "        scores.append((new_sr, sr, depth, conf))\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    \"\"\"for s in scores:\n",
    "        print(\"new_score:{} acc:{} depth:{}\".format(round(s[0], 2), s[1], s[2]))\"\"\"\n",
    "    return scores[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b168e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explainer(dataset_name, best_param, tr_set, tr_label, ts_set, ts_label, path_tuple, lss_dpt=False):\n",
    "    dt = tree.DecisionTreeClassifier(**best_param)\n",
    "    dt.fit(tr_set, tr_label)\n",
    "    predictions1 = dt.predict(tr_set)\n",
    "    report = classification_report(tr_label, predictions1)\n",
    "    write_report = open(\"./explainers/{}/{}/{}/best_param{}.txt\".format(dataset_name, path_tuple[0], path_tuple[1],\n",
    "                                                                        \"_lssdpt\" if lss_dpt else \"\"), \"w\")\n",
    "    write_report.write(str(best_param))\n",
    "    write_report.close()\n",
    "\n",
    "    write_report = open(\"./explainers/{}/{}/{}/tr_fidelity{}.txt\".format(dataset_name, path_tuple[0], path_tuple[1],\n",
    "                                                                         \"_lssdpt\" if lss_dpt else \"\"), \"w\")\n",
    "    write_report.write(report)\n",
    "    write_report.close()\n",
    "\n",
    "    predictions = dt.predict(ts_set)\n",
    "    report = classification_report(ts_label, predictions)\n",
    "\n",
    "    write_report = open(\"./explainers/{}/{}/{}/ts_fidelity{}.txt\".format(dataset_name, path_tuple[0], path_tuple[1],\n",
    "                                                                         \"_lssdpt\" if lss_dpt else \"\"), \"w\")\n",
    "    write_report.write(report)\n",
    "    write_report.close()\n",
    "\n",
    "    # Train fidelity\n",
    "    fidelity_train = dt.predict(train_set)\n",
    "    report = classification_report(train_label, fidelity_train)\n",
    "    write_report = open(\"./explainers/{}/{}/{}/tr_original{}.txt\".format(dataset_name, path_tuple[0], path_tuple[1],\n",
    "                                                                         \"_lssdpt\" if lss_dpt else \"\"), \"w\")\n",
    "    write_report.write(report)\n",
    "    write_report.close()\n",
    "\n",
    "    # Test fidelity\n",
    "    fidelity_test = dt.predict(test_set)\n",
    "    report = classification_report(test_label, fidelity_test)\n",
    "    write_report = open(\n",
    "        \"./explainers/{}/{}/{}/ts_original{}.txt\".format(dataset_name, path_tuple[0], path_tuple[1],\n",
    "                                                         \"_lssdpt\" if lss_dpt else \"\"), \"w\")\n",
    "    write_report.write(report)\n",
    "    write_report.close()\n",
    "\n",
    "    filename = \"./explainers/{}/{}/{}/explainer{}.sav\".format(dataset_name, path_tuple[0], path_tuple[1],\n",
    "                                                              \"_lssdpt\" if lss_dpt else \"\")\n",
    "    pickle.dump(dt, open(filename, 'wb'))\n",
    "    write_report.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd225d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greater_than_85(config_list, gs_scores):\n",
    "    \"\"\"\n",
    "    We select the model having accuracy greater\n",
    "    than 85 and the minimum number of depth.\n",
    "    :param config_list:\n",
    "    :param gs_scores:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    coupled = [(gs_scores[i], conf['max_depth'], conf) for i, conf in enumerate(config_list)]\n",
    "    filtered = list(filter(lambda x: x[0] >= 0.77, coupled))\n",
    "    filtered.sort(key=lambda x: x[0], reverse=False)\n",
    "    print(filtered)\n",
    "    return filtered[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f900d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explainer_regularized(ds_name, train_set, train_label, test_set, test_label, path_tuple, max_depth_l=[8, 9, 10, 11, 12, 13]):\n",
    "    tree_para = {'criterion': ['gini', 'entropy'], 'max_depth': max_depth_l,\n",
    "                 'min_samples_split': [5, 10, 15, 25, 30, 50], 'min_samples_leaf': [3, 5, 15, 20, 40, 50],\n",
    "                 'max_features': [5, 'auto', 'sqrt', 'log2']}\n",
    "    grid = GridSearchCV(tree.DecisionTreeClassifier(), tree_para, cv=3, n_jobs=12, verbose=10, scoring='accuracy')\n",
    "    grid.fit(train_set, train_label.ravel())\n",
    "    configurations_list = grid.cv_results_['params']\n",
    "    scores_list = grid.cv_results_['mean_test_score']\n",
    "    # Select the way you want to perform the model selection\n",
    "    best_param = new_score_sorting(configurations_list, scores_list)\n",
    "    # best_param = greater_than_85(configurations_list, scores_list)\n",
    "    # best_param = grid.best_params_\n",
    "    # normal training of the explainer\n",
    "    train_explainer(ds_name, best_param, train_set, train_label, test_set, test_label, path_tuple, lss_dpt=True)\n",
    "    # train_explainer_for_plot(best_param, train_set, train_label, test_set, test_label, max_depth_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7867fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizeds = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "254e8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_hyperparams = [\n",
    "    {'criterion': 'entropy', 'max_depth': 20, 'max_features': 5, 'min_samples_leaf': 40, 'min_samples_split': 50},\n",
    "    {'criterion': 'gini', 'max_depth': 13, 'splitter': 'best', 'max_features': None, 'min_samples_leaf': 1,\n",
    "     'min_samples_split': 2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8884bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train0= pd.read_csv('training data path')\n",
    "test0 = pd.read_csv('testing data path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e840db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a07a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pytorch_prediction(bb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a921822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4320 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:269: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:269: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      " 50%|█████████████████████████████████████████                                         | 1/2 [22:00<22:00, 1320.55s/it]C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4320 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:269: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\012709558\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [36:45<00:00, 1102.57s/it]\n"
     ]
    }
   ],
   "source": [
    "ds_names = ['Adult']\n",
    "regularizeds = [False, True]\n",
    "dt_best_hyperparams = [\n",
    "    {'criterion': 'entropy', 'max_depth': 20, 'max_features': 5, 'min_samples_leaf': 40, 'min_samples_split': 50},\n",
    "    {'criterion': 'gini', 'max_depth': 13, 'splitter': 'best', 'max_features': None, 'min_samples_leaf': 1,\n",
    "     'min_samples_split': 2},\n",
    "]\n",
    "# For each dataset we have\n",
    "for i, ds_name in enumerate(ds_names):\n",
    "    # We load the dataset data\n",
    "    train_set = train0.iloc[:,:39]\n",
    "    test_set = test0.iloc[:,:39]\n",
    "    train_label = train0.iloc[:,39:]\n",
    "    test_label = test0.iloc[:,39:]\n",
    "    bboxes = []\n",
    "    # Here we allocate the 6 different bboxes\n",
    "    for regularized in regularizeds:\n",
    "        # dt = DecisionTreeBlackBox(db_name=ds_name, regularized=regularized)\n",
    "        # rf = RandomForestBlackBox(db_name=ds_name, regularized=regularized)\n",
    "        nn = model\n",
    "        bboxes.append(nn)\n",
    "    for bb in tqdm(bboxes):\n",
    "        # Here we generate the new data according to Trepan\n",
    "        generator = TrePanGenerator()\n",
    "        gen = generator.generate(train_set.values, oracle=bb, size=80000)\n",
    "        data_l = gen[:, -1]\n",
    "        data = np.delete(gen, -1, axis=1)\n",
    "        tr_set, ts_set, tr_label, ts_label = train_test_split(data, data_l, stratify=data_l,\n",
    "                                                              test_size=0.20, random_state=0)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"path to save the data\")\n",
    "        df = pd.DataFrame(data_l)\n",
    "        df.to_csv(\"path to save teh labels\")\n",
    "        \n",
    "        # Best params\n",
    "        train_explainer(ds_name, dt_best_hyperparams[1], tr_set, tr_label, ts_set, ts_label,\n",
    "                        (\"nn\", \"regularized\"))\n",
    "        # Regolarized with less depth\n",
    "        train_explainer_regularized(ds_name, tr_set, tr_label, ts_set, ts_label,\n",
    "                                        (\"nn\", \"regularized\"), max_depth_l=[i for i in range(5, 20)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1295773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27aabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b548549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
