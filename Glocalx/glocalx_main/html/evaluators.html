<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>evaluators API documentation</title>
<meta name="description" content="Evaluation module providing basic metrics to run and analyze GLocalX&#39;s results.
Two evaluators are provided, DummyEvaluator, which does not optimize â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>evaluators</code></h1>
</header>
<section id="section-intro">
<p>Evaluation module providing basic metrics to run and analyze GLocalX's results.
Two evaluators are provided, DummyEvaluator, which does not optimize performance,
and MemEvaluator, which stores previously computed measures to speed-up performance.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Evaluation module providing basic metrics to run and analyze GLocalX&#39;s results.
Two evaluators are provided, DummyEvaluator, which does not optimize performance,
and MemEvaluator, which stores previously computed measures to speed-up performance.
&#34;&#34;&#34;
from abc import abstractmethod

import numpy as np
from scipy.spatial.distance import hamming

from logzero import logger

from models import Rule


def covers(rule, x):
    &#34;&#34;&#34;Does `rule` cover c?

    Args:
        rule (Rule): The rule.
        x (numpy.np.array): The record.
    Returns:
        bool: True if this rule covers c, False otherwise.
    &#34;&#34;&#34;
    return all([(x[feature] &gt;= lower) &amp; (x[feature] &lt; upper)] for feature, (lower, upper) in rule)


def binary_fidelity(unit, x, y, evaluator=None, ids=None, default=np.nan):
    &#34;&#34;&#34;Evaluate the goodness of unit.
    Args:
        unit (Unit): The unit to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        evaluator (Evaluator): Optional evaluator to speed-up computation.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        default (int): Default prediction for records not covered by the unit.
    Returns:
          float: The unit&#39;s fidelity_weight
    &#34;&#34;&#34;
    coverage = evaluator.coverage(unit, x, ids=ids).flatten()
    unit_predictions = np.array([unit.consequence
                                 for _ in range(x.shape[0] if ids is None else ids.shape[0])]).flatten()
    unit_predictions[~coverage] = default

    fidelity = 1 - hamming(unit_predictions, y[ids] if ids is not None else y) if len(y) &gt; 0 else 0

    return fidelity


def coverage_size(rule, x):
    &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

    Args:
        rule (Rule): The rule.
        x (numpy.array): The validation set.

    Returns:
        (int: Number of records of X covered by rule.
    &#34;&#34;&#34;
    return coverage_matrix([rule], x).sum().item(0)


def coverage_matrix(rules, patterns, targets=None, ids=None):
    &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
    Args:
        rules (Union(list, Rule)): List of rules (or single Rule) whose coverage to compute.
        patterns (numpy.array): The validation set.
        targets (numpy.array): The labels, if any. None otherwise. Defaults to None.
        ids (numpy.array): Unique identifiers to tell each element in `x` apart.
    Returns:
        numpy.array: The coverage matrix.
    &#34;&#34;&#34;
    def premises_from(rule, pure=False):
        if not pure:
            premises = np.logical_and.reduce([[(patterns[:, feature] &gt; lower) &amp; (patterns[:, feature] &lt;= upper)]
                                              for feature, (lower, upper) in rule]).squeeze()
        else:
            premises = np.logical_and.reduce([(patterns[:, feature] &gt; lower) &amp; (patterns[:, feature] &lt;= upper)
                                              &amp; (targets == rule.consequence)
                                              for feature, (lower, upper) in rule]).squeeze()

        premises = np.argwhere(premises).squeeze()

        return premises

    if isinstance(rules, list):
        coverage_matrix_ = np.full((len(rules), len(patterns)), False)
        hit_columns = [premises_from(rule, targets is not None) for rule in rules]

        for k, hits in zip(range(len(patterns)), hit_columns):
            coverage_matrix_[k, hits] = True
    else:
        coverage_matrix_ = np.full((len(patterns)), False)
        hit_columns = [premises_from(rules, targets is not None)]
        coverage_matrix_[hit_columns] = True

    coverage_matrix_ = coverage_matrix_[:, ids] if ids is not None else coverage_matrix_

    return coverage_matrix_


class Evaluator:
    &#34;&#34;&#34;Evaluator interface. Evaluator objects provide coverage and fidelity_weight utilities.&#34;&#34;&#34;

    @abstractmethod
    def coverage(self, rules, patterns, target=None, ids=None):
        &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
        Args:
            rules (list) or (Rule):
            patterns (numpy.array): The validation set.
            target (numpy.array): The labels, if any. None otherwise. Defaults to None.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
            numpy.array: The coverage matrix.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def coverage_size(self, rule, x, ids=None):
        &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

        Args:
            rule (Rule): The rule.
            x (numpy.array): The validation set.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
            int: Number of records of X covered by rule.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def binary_fidelity(self, unit, x, y, ids=None, default=np.nan):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            unit (Unit): The unit to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
            default (int): Default prediction when no rule covers a record.
        Returns:
              float: The unit&#39;s fidelity_weight
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of the `units`.
        Args:
            units (Union(list, set)): The units to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            k (int): Number of rules to use in the Laplacian prediction schema.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): Unique identifiers to tell each element in @c apart.
        Returns:
            float: The units fidelity_weight.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def covers(self, rule, x):
        &#34;&#34;&#34;Does @rule cover c?

        Args:
            rule (Rule): The rule.
            x (numpy.array): The record.
        Returns:
            bool: True if this rule covers c, False otherwise.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
        &#34;&#34;&#34;
        Compute the Bayesian Information Criterion for the given `rules` set.
        Args:
            rules (set): Ruleset.
            vl (numpy.array): Validation set.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
            complexity_weight (float): Weight to complexity_weight (BIC-wise).
        Returns:
            tuple: Triple (BIC, log likelihood, complexity_weight).
        &#34;&#34;&#34;
        pass


class DummyEvaluator(Evaluator):
    &#34;&#34;&#34;Dummy evaluator with no memory: every result is computed at each call!&#34;&#34;&#34;

    def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
        &#34;&#34;&#34;
        Compute the Bayesian Information Criterion for the given `rules` set.
        Args:
            rules (set): Ruleset.
            vl (numpy.array): Validation set.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
            complexity_weight (float): Weight to complexity_weight (BIC-wise).
        Returns:
            tuple: Triple (BIC, log likelihood, complexity_weight).
        &#34;&#34;&#34;
        x, y = vl[:, :-1], vl[:, -1]
        n = x.shape[0]
        default = round(y.mean() + .5)
        log_likelihood = [binary_fidelity(rule, x, y, default=default, ids=None) for rule in rules]
        log_likelihood = np.mean(log_likelihood)

        model_complexity = len(rules)
        model_bic = - (fidelity_weight * log_likelihood - complexity_weight * model_complexity / n)

        return model_bic, log_likelihood, model_complexity

    def __init__(self, oracle):
        &#34;&#34;&#34;Constructor.&#34;&#34;&#34;
        self.oracle = oracle
        self.coverages = dict()
        self.binary_fidelities = dict()
        self.coverage_sizes = dict()

    def covers(self, rule, x):
        &#34;&#34;&#34;Does `rule` cover `x`?

        Args:
            rule (Rule): The rule.
            x (numpy.array): The record.
        Returns:
            bool: True if this rule covers c, False otherwise.
        &#34;&#34;&#34;
        return covers(rule, x)

    def coverage(self, rules, patterns, target=None, ids=None):
        &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
        Args:
            rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.
            patterns (numpy.array): The validation set.
            target (numpy.array): The labels, if any. None otherwise. Defaults to None.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
            numpy.array: The coverage matrix.
        &#34;&#34;&#34;
        rules_ = rules if isinstance(rules, list) else [rules]
        coverage_ = coverage_matrix(rules_, patterns, target, ids=ids)

        return coverage_

    def coverage_size(self, rule, x, ids=None):
        &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

        Args:
            rule (Rule): The rule.
            x (numpy.array): The validation set.
            ids (numpy.array): Unique identifiers to tell each element in `x` apart.
        Returns:
            numpy.array: Number of records of X covered by rule.
        &#34;&#34;&#34;
        return coverage_size(rule, x)

    def binary_fidelity(self, unit, x, y, default=np.nan, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            unit (Unit): The unit to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
            default (int): Default prediction when no rule covers a record.
        Returns:
              (float): The unit&#39;s fidelity_weight
        &#34;&#34;&#34;
        if self.oracle is not None or y is None:
            y = self.oracle.predict(x).round().squeeze()

        return binary_fidelity(unit, x, y, self, default=default, ids=ids)

    def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            units (Union(list, set): The units to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            k (int): Number of rules to use in the Laplacian prediction schema.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
              numpy.array: The units fidelity_weight.
        &#34;&#34;&#34;
        if self.oracle is not None:
            y = (self.oracle.predict(x).round().squeeze())

        scores = np.array([self.binary_fidelity(rule, x, y, default=default) for rule in units])
        coverage = self.coverage(units, x, y)

        predictions = []
        for record in range(len(x)):
            companions = scores[coverage[:, record]]
            companion_units = units[coverage[:, record]]
            top_companions = np.argsort(companions)[-k:]
            top_units = companion_units[top_companions]
            top_fidelities = companions[top_companions]
            top_fidelities_0 = [top_fidelity for top_fidelity, top_unit in zip(top_fidelities, top_units)
                                if top_unit.consequence == 0]
            top_fidelities_1 = [top_fidelity for top_fidelity, top_unit in zip(top_fidelities, top_units)
                                if top_unit.consequence == 1]

            if len(top_fidelities_0) == 0 and len(top_fidelities_1) &gt; 0:
                prediction = 1
            elif len(top_fidelities_1) == 0 and len(top_fidelities_0) &gt; 0:
                prediction = 0
            elif len(top_fidelities_1) == 0 and len(top_fidelities_0) == 0:
                prediction = default
            else:
                prediction = 0 if np.mean(top_fidelities_0) &gt; np.mean(top_fidelities_1) else 1

            predictions.append(prediction)
        predictions = np.array(predictions)
        fidelity = 1 - hamming(predictions, y) if len(y) &gt; 0 else 0

        return fidelity


class MemEvaluator(Evaluator):
    &#34;&#34;&#34;Memoization-aware Evaluator to avoid evaluating the same measures over the same data.&#34;&#34;&#34;

    def __init__(self, oracle):
        &#34;&#34;&#34;Constructor.&#34;&#34;&#34;
        self.oracle = oracle
        self.coverages = dict()
        self.perfect_coverages = dict()
        self.intersecting = dict()
        self.bics = dict()
        self.distances = dict()
        self.binary_fidelities = dict()
        self.coverage_sizes = dict()
        self.scores = dict()

    @abstractmethod
    def covers(self, rule, x):
        &#34;&#34;&#34;Does @rule cover c?

        Args:
            rule (Rule): The rule.
            x (numpy.array): The record.
        Returns:
            bool: True if this rule covers c, False otherwise.
        &#34;&#34;&#34;
        return covers(rule, x)

    def coverage(self, rules, patterns, targets=None, ids=None):
        &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
        Args:
            rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.
            patterns (numpy.array): The validation set.
            targets (numpy.array): The labels, if any. None otherwise. Defaults to None.
            ids (numpy.array): IDS of the given `patterns`, used to speed up evaluation.
        Returns:
            numpy.array: The coverage matrix.
        &#34;&#34;&#34;
        rules_ = [rules] if not isinstance(rules, list) and not isinstance(rules, set) else rules

        if targets is None:
            for rule in rules_:
                if rule not in self.coverages:
                    self.coverages[rule] = coverage_matrix(rule, patterns, targets)
            cov = np.array([self.coverages[rule] for rule in rules_])
        else:
            for rule in rules_:
                if rule not in self.perfect_coverages:
                    self.perfect_coverages[rule] = coverage_matrix(rule, patterns, targets)
            cov = np.array([self.perfect_coverages[rule] for rule in rules_])

        cov = cov[:, ids] if ids is not None else cov

        return cov

    def distance(self, A, B, x, ids=None):
        &#34;&#34;&#34;
        Compute the distance between ruleset `A` and ruleset `B`.
        Args:
            A (iterable): Ruleset.
            B (iterable): Ruleset.
            x (numpy.array): Data.
            ids (numpy.array): IDS of the given `x`, used to speed up evaluation.
        Returns:
            (float): The Jaccard distance between the two.
        &#34;&#34;&#34;
        if tuple(A) in self.distances and tuple(B) in self.distances[tuple(A)]:
            diff = self.distances[tuple(A)][tuple(B)]
            return diff
        if tuple(B) in self.distances and tuple(A) in self.distances[tuple(B)]:
            diff = self.distances[tuple(B)][tuple(A)]
            return diff

        # New distance to compute
        coverage_A, coverage_B = self.coverage(A, x, ids=ids).sum(axis=0), self.coverage(B, x, ids=ids).sum(axis=0)
        diff = hamming(coverage_A, coverage_B)
        if tuple(A) in self.distances:
            self.distances[tuple(A)][tuple(B)] = diff
        if tuple(B) in self.distances:
            self.distances[tuple(B)][tuple(A)] = diff

        # First time for A
        if tuple(A) not in self.distances:
            self.distances[tuple(A)] = {tuple(B): diff}
        # First time for B
        if tuple(B) not in self.distances:
            self.distances[tuple(B)] = {tuple(A): diff}

        return diff

    def binary_fidelity(self, unit, x, y, default=np.nan, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            unit (Unit): The unit to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): IDS of the given `x`, used to speed up evaluation.
        Returns:
              float: The unit&#39;s fidelity_weight
        &#34;&#34;&#34;
        if y is None:
            y = self.oracle.predict(x).round().squeeze()

        if ids is None:
            self.binary_fidelities[unit] = self.binary_fidelities.get(unit, binary_fidelity(unit, x, y, self,
                                                                                            default=default, ids=None))
            fidelity = self.binary_fidelities[unit]
        else:
            fidelity = binary_fidelity(unit, x, y, self, default=default, ids=ids)

        return fidelity

    def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of the `units`.
        Args:
            units (Union(list, set)): The units to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            k (int): Number of rules to use in the Laplacian prediction schema.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): Unique identifiers to tell each element in @c apart.
        Returns:
              float: The units fidelity_weight.
        &#34;&#34;&#34;
        if y is None:
            y = self.oracle.predict(x).squeeze().round()

        scores = np.array([self.binary_fidelity(rule, x, y, default=default) for rule in units])
        coverage = self.coverage(units, x)

        if len(units) == 0:
            predictions = [default] * y.shape[0]
        else:
            rules_consequences = np.array([r.consequence for r in units])
            # Fast computation for k = 1
            if k == 1:
                weighted_coverage_scores = coverage * scores.reshape(-1, 1)  # Coverage matrix weighted by score
                # Best score per row (i.e., record)
                best_rule_per_record_idx = weighted_coverage_scores.argmax(axis=0).squeeze()
                predictions = rules_consequences[best_rule_per_record_idx]
                # Replace predictions of non-covered records w/ default prediction
                predictions[coverage.sum(axis=0) == 0] = default
            # Iterative computation
            else:
                predictions = []
                for record in range(len(x)):
                    record_coverage = np.argwhere(coverage[:, record]).ravel()
                    if len(record_coverage) == 0:
                        prediction = default
                    else:
                        companions_0 = record_coverage[rules_consequences[record_coverage] == 0]
                        companions_1 = record_coverage[rules_consequences[record_coverage] == 1]
                        scores_0 = scores[companions_0]
                        scores_1 = scores[companions_1]
                        np.argsort_scores_0 = np.flip(np.argsort(scores[companions_0])[-k:])
                        np.argsort_scores_1 = np.flip(np.argsort(scores[companions_1])[-k:])
                        top_scores_0 = scores_0[np.argsort_scores_0]
                        top_scores_1 = scores_1[np.argsort_scores_1]

                        if len(top_scores_0) == 0 and len(top_scores_1) &gt; 0:
                            prediction = 1
                        elif len(top_scores_1) == 0 and len(top_scores_0) &gt; 0:
                            prediction = 0
                        elif len(top_scores_1) == 0 and len(top_scores_0) == 0:
                            prediction = default
                        else:
                            prediction = 0 if np.mean(top_scores_0) &gt; np.mean(top_scores_1) else 1

                    predictions.append(prediction)
                predictions = np.array(predictions)
        fidelity = 1 - hamming(predictions, y) if len(y) &gt; 0 else 0

        return fidelity

    def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
        &#34;&#34;&#34;
        Compute the Bayesian Information Criterion for the given `rules` set.
        Args:
            rules (set): Ruleset.
            vl (numpy.array): Validation set.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
            complexity_weight (float): Weight to complexity_weight (BIC-wise).
        Returns:
            float: Model BIC 
        &#34;&#34;&#34;
        if tuple(rules) in self.bics:
            model_bic = self.bics[tuple(rules)]
        else:
            x, y = vl[:, :-1], vl[:, -1]
            n, m = x.shape
            default = int(y.mean().round())
            log_likelihood = self.binary_fidelity_model(rules, x, y, default=default)

            model_complexity = np.mean([len(r) / m for r in rules])
            model_bic = - (fidelity_weight * log_likelihood - complexity_weight * model_complexity / n)

            logger.debug(&#39;Log likelihood: &#39; + str(log_likelihood) + &#39; | Complexity: &#39; + str(model_complexity))

            self.bics[tuple(rules)] = model_bic

        return model_bic

    def forget(self, rules, A=None, B=None):
        &#34;&#34;&#34;
        Remove rules from this Evaluator&#39;s memory. Return the updated evaluator.
        Args:
            rules (iterable): Rules to remove.
            A (set): Rules merged.
            B (set): Rules merged.
        Returns:
            MemEvaluator: This evaluator with no memory of `rules`.

        &#34;&#34;&#34;
        for rule in rules:
            if rule in self.binary_fidelities:
                del self.binary_fidelities[rule]
            if rule in self.coverages:
                del self.coverages[rule]
            if rule in self.coverage_sizes:
                del self.coverage_sizes[rule]
            if rule in self.perfect_coverages:
                del self.perfect_coverages[rule]
            if rule in self.scores:
                del self.scores[rule]

        if A is not None and B is not None:
            # Delete the whole A, as it has been merged and does not exist anymore
            del self.distances[tuple(A)]
            # Delete the whole B, as it has been merged and does not exist anymore
            del self.distances[tuple(B)]
            # Delete every reference to any of them, as they have been merged and do not exist anymore
            for T in self.distances:
                if tuple(A) in self.distances[T]:
                    del self.distances[T][tuple(A)]
                if tuple(B) in self.distances[T]:
                    del self.distances[T][tuple(B)]

        return self


########################
# Framework validation #
########################
def validate(glocalx, oracle, vl, m=None, alpha=0.5, is_percentile=False):
    &#34;&#34;&#34;Validate the given `glocalx` instance on the given `tr` dataset.
    Arguments:
        glocalx (Union(GLocalX, list)): GLocalX object or list of rules.
        oracle (Predictor): Oracle to validate against.
        vl (numpy.array): Validation set.
        m (int): Initial number of rules, if known, None otherwise. Defaults to None.
        alpha (Union(float, int, iterable)): Pruning hyperparameter, rules with score
                                            less than `alpha` are removed from the ruleset
                                            used to perform the validation. The score can be
                                            - rule fidelity (`alpha` float, `is_percentile`=False)
                                            - rule fidelity percentile (`alpha` float, `is_percentile`=True)
                                            - number of rules (`alpha` integer)
                                            Same applies if an iterable is provided.
                                            Defaults to &#39;0.5&#39;.
        is_percentile (bool): Whether the provided `alpha` is a percentile or not. Defaults to False.
    Returns:
        dict: Dictionary of validation measures.
    &#34;&#34;&#34;
    def len_reduction(ruleset_a, ruleset_b):
        return ruleset_a / ruleset_b

    def coverage_pct(rules, x):
        coverage = coverage_matrix(rules, x)
        coverage_percentage = (coverage.sum(axis=0) &gt; 0).sum() / x.shape[0]

        return coverage_percentage

    if oracle is None:
        x = vl[:, :-1]
        y = vl[:, -1]
        evaluator = MemEvaluator(oracle=None)
    else:
        evaluator = MemEvaluator(oracle=oracle)
        x = vl[:, :-1]
        y = oracle.predict(x).round().squeeze()
    majority_label = int(y.mean().round())

    if isinstance(alpha, float) or isinstance(alpha, int):
        alphas = [alpha]
    else:
        alphas = alpha

    results = {}
    for alpha in alphas:
        if isinstance(glocalx, list) or isinstance(glocalx, set):
            rules = glocalx
        else:
            if oracle is None:
                evaluator = MemEvaluator(oracle=None)
            rules = glocalx.rules(alpha=alpha, data=np.hstack((x, y.reshape(-1, 1))),
                                  evaluator=evaluator, is_percentile=is_percentile)
        rules = [r for r in rules if len(r) &gt; 0 and isinstance(r, Rule)]

        if len(rules) == 0:
            results[alpha] = {
                &#39;alpha&#39;: alpha,
                &#39;fidelity&#39;: np.nan,
                &#39;fidelity_weight&#39;: np.nan,
                &#39;coverage&#39;: np.nan,
                &#39;mean_length&#39;: np.nan,
                &#39;std_length&#39;: np.nan,
                &#39;rule_reduction&#39;: np.nan,
                &#39;len_reduction&#39;: np.nan,
                &#39;mean_prediction&#39;: np.nan,
                &#39;std_prediction&#39;: np.nan,
                &#39;size&#39;: 0
            }
            continue

        evaluator = MemEvaluator(oracle=oracle)
        validation = dict()
        validation[&#39;alpha&#39;] = alpha
        validation[&#39;size&#39;] = len(rules)
        validation[&#39;fidelity&#39;] = evaluator.binary_fidelity_model(rules, x=x, y=y, default=majority_label, k=1)
        validation[&#39;coverage&#39;] = coverage_pct(rules, x)
        validation[&#39;mean_length&#39;] = np.mean([len(r) for r in rules])
        validation[&#39;std_length&#39;] = np.std([len(r) for r in rules])
        validation[&#39;rule_reduction&#39;] = 1 - len(rules) / m if m is not None else np.nan
        validation[&#39;len_reduction&#39;] = len_reduction(validation[&#39;mean_length&#39;], m) if m is not None else np.nan

        # Predictions
        validation[&#39;mean_prediction&#39;] = np.mean([r.consequence for r in rules])
        validation[&#39;std_prediction&#39;] = np.std([r.consequence for r in rules])

        results[alpha] = validation

    return results</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="evaluators.binary_fidelity"><code class="name flex">
<span>def <span class="ident">binary_fidelity</span></span>(<span>unit, x, y, evaluator=None, ids=None, default=nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the goodness of unit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unit</code></strong> :&ensp;<code>Unit</code></dt>
<dd>The unit to evaluate.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels.</dd>
<dt><strong><code>evaluator</code></strong> :&ensp;<code><a title="evaluators.Evaluator" href="#evaluators.Evaluator">Evaluator</a></code></dt>
<dd>Optional evaluator to speed-up computation.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @patterns apart.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>int</code></dt>
<dd>Default prediction for records not covered by the unit.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The unit's fidelity_weight</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def binary_fidelity(unit, x, y, evaluator=None, ids=None, default=np.nan):
    &#34;&#34;&#34;Evaluate the goodness of unit.
    Args:
        unit (Unit): The unit to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        evaluator (Evaluator): Optional evaluator to speed-up computation.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        default (int): Default prediction for records not covered by the unit.
    Returns:
          float: The unit&#39;s fidelity_weight
    &#34;&#34;&#34;
    coverage = evaluator.coverage(unit, x, ids=ids).flatten()
    unit_predictions = np.array([unit.consequence
                                 for _ in range(x.shape[0] if ids is None else ids.shape[0])]).flatten()
    unit_predictions[~coverage] = default

    fidelity = 1 - hamming(unit_predictions, y[ids] if ids is not None else y) if len(y) &gt; 0 else 0

    return fidelity</code></pre>
</details>
</dd>
<dt id="evaluators.coverage_matrix"><code class="name flex">
<span>def <span class="ident">coverage_matrix</span></span>(<span>rules, patterns, targets=None, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the coverage of @rules over @patterns.</p>
<h2 id="args">Args</h2>
<dl>
<dt>rules (Union(list, Rule)): List of rules (or single Rule) whose coverage to compute.</dt>
<dt><strong><code>patterns</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The validation set.</dd>
<dt><strong><code>targets</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels, if any. None otherwise. Defaults to None.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in <code>x</code> apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>The coverage matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coverage_matrix(rules, patterns, targets=None, ids=None):
    &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
    Args:
        rules (Union(list, Rule)): List of rules (or single Rule) whose coverage to compute.
        patterns (numpy.array): The validation set.
        targets (numpy.array): The labels, if any. None otherwise. Defaults to None.
        ids (numpy.array): Unique identifiers to tell each element in `x` apart.
    Returns:
        numpy.array: The coverage matrix.
    &#34;&#34;&#34;
    def premises_from(rule, pure=False):
        if not pure:
            premises = np.logical_and.reduce([[(patterns[:, feature] &gt; lower) &amp; (patterns[:, feature] &lt;= upper)]
                                              for feature, (lower, upper) in rule]).squeeze()
        else:
            premises = np.logical_and.reduce([(patterns[:, feature] &gt; lower) &amp; (patterns[:, feature] &lt;= upper)
                                              &amp; (targets == rule.consequence)
                                              for feature, (lower, upper) in rule]).squeeze()

        premises = np.argwhere(premises).squeeze()

        return premises

    if isinstance(rules, list):
        coverage_matrix_ = np.full((len(rules), len(patterns)), False)
        hit_columns = [premises_from(rule, targets is not None) for rule in rules]

        for k, hits in zip(range(len(patterns)), hit_columns):
            coverage_matrix_[k, hits] = True
    else:
        coverage_matrix_ = np.full((len(patterns)), False)
        hit_columns = [premises_from(rules, targets is not None)]
        coverage_matrix_[hit_columns] = True

    coverage_matrix_ = coverage_matrix_[:, ids] if ids is not None else coverage_matrix_

    return coverage_matrix_</code></pre>
</details>
</dd>
<dt id="evaluators.coverage_size"><code class="name flex">
<span>def <span class="ident">coverage_size</span></span>(<span>rule, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the cardinality of the coverage of unit on c.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rule</code></strong> :&ensp;<code>Rule</code></dt>
<dd>The rule.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The validation set.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(int: Number of records of X covered by rule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coverage_size(rule, x):
    &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

    Args:
        rule (Rule): The rule.
        x (numpy.array): The validation set.

    Returns:
        (int: Number of records of X covered by rule.
    &#34;&#34;&#34;
    return coverage_matrix([rule], x).sum().item(0)</code></pre>
</details>
</dd>
<dt id="evaluators.covers"><code class="name flex">
<span>def <span class="ident">covers</span></span>(<span>rule, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Does <code>rule</code> cover c?</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rule</code></strong> :&ensp;<code>Rule</code></dt>
<dd>The rule.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.np.array</code></dt>
<dd>The record.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if this rule covers c, False otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def covers(rule, x):
    &#34;&#34;&#34;Does `rule` cover c?

    Args:
        rule (Rule): The rule.
        x (numpy.np.array): The record.
    Returns:
        bool: True if this rule covers c, False otherwise.
    &#34;&#34;&#34;
    return all([(x[feature] &gt;= lower) &amp; (x[feature] &lt; upper)] for feature, (lower, upper) in rule)</code></pre>
</details>
</dd>
<dt id="evaluators.validate"><code class="name flex">
<span>def <span class="ident">validate</span></span>(<span>glocalx, oracle, vl, m=None, alpha=0.5, is_percentile=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Validate the given <code>glocalx</code> instance on the given <code>tr</code> dataset.</p>
<h2 id="arguments">Arguments</h2>
<p>glocalx (Union(GLocalX, list)): GLocalX object or list of rules.
oracle (Predictor): Oracle to validate against.
vl (numpy.array): Validation set.
m (int): Initial number of rules, if known, None otherwise. Defaults to None.
alpha (Union(float, int, iterable)): Pruning hyperparameter, rules with score
less than <code>alpha</code> are removed from the ruleset
used to perform the validation. The score can be
- rule fidelity (<code>alpha</code> float, <code>is_percentile</code>=False)
- rule fidelity percentile (<code>alpha</code> float, <code>is_percentile</code>=True)
- number of rules (<code>alpha</code> integer)
Same applies if an iterable is provided.
Defaults to '0.5'.
is_percentile (bool): Whether the provided <code>alpha</code> is a percentile or not. Defaults to False.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary of validation measures.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate(glocalx, oracle, vl, m=None, alpha=0.5, is_percentile=False):
    &#34;&#34;&#34;Validate the given `glocalx` instance on the given `tr` dataset.
    Arguments:
        glocalx (Union(GLocalX, list)): GLocalX object or list of rules.
        oracle (Predictor): Oracle to validate against.
        vl (numpy.array): Validation set.
        m (int): Initial number of rules, if known, None otherwise. Defaults to None.
        alpha (Union(float, int, iterable)): Pruning hyperparameter, rules with score
                                            less than `alpha` are removed from the ruleset
                                            used to perform the validation. The score can be
                                            - rule fidelity (`alpha` float, `is_percentile`=False)
                                            - rule fidelity percentile (`alpha` float, `is_percentile`=True)
                                            - number of rules (`alpha` integer)
                                            Same applies if an iterable is provided.
                                            Defaults to &#39;0.5&#39;.
        is_percentile (bool): Whether the provided `alpha` is a percentile or not. Defaults to False.
    Returns:
        dict: Dictionary of validation measures.
    &#34;&#34;&#34;
    def len_reduction(ruleset_a, ruleset_b):
        return ruleset_a / ruleset_b

    def coverage_pct(rules, x):
        coverage = coverage_matrix(rules, x)
        coverage_percentage = (coverage.sum(axis=0) &gt; 0).sum() / x.shape[0]

        return coverage_percentage

    if oracle is None:
        x = vl[:, :-1]
        y = vl[:, -1]
        evaluator = MemEvaluator(oracle=None)
    else:
        evaluator = MemEvaluator(oracle=oracle)
        x = vl[:, :-1]
        y = oracle.predict(x).round().squeeze()
    majority_label = int(y.mean().round())

    if isinstance(alpha, float) or isinstance(alpha, int):
        alphas = [alpha]
    else:
        alphas = alpha

    results = {}
    for alpha in alphas:
        if isinstance(glocalx, list) or isinstance(glocalx, set):
            rules = glocalx
        else:
            if oracle is None:
                evaluator = MemEvaluator(oracle=None)
            rules = glocalx.rules(alpha=alpha, data=np.hstack((x, y.reshape(-1, 1))),
                                  evaluator=evaluator, is_percentile=is_percentile)
        rules = [r for r in rules if len(r) &gt; 0 and isinstance(r, Rule)]

        if len(rules) == 0:
            results[alpha] = {
                &#39;alpha&#39;: alpha,
                &#39;fidelity&#39;: np.nan,
                &#39;fidelity_weight&#39;: np.nan,
                &#39;coverage&#39;: np.nan,
                &#39;mean_length&#39;: np.nan,
                &#39;std_length&#39;: np.nan,
                &#39;rule_reduction&#39;: np.nan,
                &#39;len_reduction&#39;: np.nan,
                &#39;mean_prediction&#39;: np.nan,
                &#39;std_prediction&#39;: np.nan,
                &#39;size&#39;: 0
            }
            continue

        evaluator = MemEvaluator(oracle=oracle)
        validation = dict()
        validation[&#39;alpha&#39;] = alpha
        validation[&#39;size&#39;] = len(rules)
        validation[&#39;fidelity&#39;] = evaluator.binary_fidelity_model(rules, x=x, y=y, default=majority_label, k=1)
        validation[&#39;coverage&#39;] = coverage_pct(rules, x)
        validation[&#39;mean_length&#39;] = np.mean([len(r) for r in rules])
        validation[&#39;std_length&#39;] = np.std([len(r) for r in rules])
        validation[&#39;rule_reduction&#39;] = 1 - len(rules) / m if m is not None else np.nan
        validation[&#39;len_reduction&#39;] = len_reduction(validation[&#39;mean_length&#39;], m) if m is not None else np.nan

        # Predictions
        validation[&#39;mean_prediction&#39;] = np.mean([r.consequence for r in rules])
        validation[&#39;std_prediction&#39;] = np.std([r.consequence for r in rules])

        results[alpha] = validation

    return results</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="evaluators.DummyEvaluator"><code class="flex name class">
<span>class <span class="ident">DummyEvaluator</span></span>
<span>(</span><span>oracle)</span>
</code></dt>
<dd>
<div class="desc"><p>Dummy evaluator with no memory: every result is computed at each call!</p>
<p>Constructor.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DummyEvaluator(Evaluator):
    &#34;&#34;&#34;Dummy evaluator with no memory: every result is computed at each call!&#34;&#34;&#34;

    def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
        &#34;&#34;&#34;
        Compute the Bayesian Information Criterion for the given `rules` set.
        Args:
            rules (set): Ruleset.
            vl (numpy.array): Validation set.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
            complexity_weight (float): Weight to complexity_weight (BIC-wise).
        Returns:
            tuple: Triple (BIC, log likelihood, complexity_weight).
        &#34;&#34;&#34;
        x, y = vl[:, :-1], vl[:, -1]
        n = x.shape[0]
        default = round(y.mean() + .5)
        log_likelihood = [binary_fidelity(rule, x, y, default=default, ids=None) for rule in rules]
        log_likelihood = np.mean(log_likelihood)

        model_complexity = len(rules)
        model_bic = - (fidelity_weight * log_likelihood - complexity_weight * model_complexity / n)

        return model_bic, log_likelihood, model_complexity

    def __init__(self, oracle):
        &#34;&#34;&#34;Constructor.&#34;&#34;&#34;
        self.oracle = oracle
        self.coverages = dict()
        self.binary_fidelities = dict()
        self.coverage_sizes = dict()

    def covers(self, rule, x):
        &#34;&#34;&#34;Does `rule` cover `x`?

        Args:
            rule (Rule): The rule.
            x (numpy.array): The record.
        Returns:
            bool: True if this rule covers c, False otherwise.
        &#34;&#34;&#34;
        return covers(rule, x)

    def coverage(self, rules, patterns, target=None, ids=None):
        &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
        Args:
            rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.
            patterns (numpy.array): The validation set.
            target (numpy.array): The labels, if any. None otherwise. Defaults to None.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
            numpy.array: The coverage matrix.
        &#34;&#34;&#34;
        rules_ = rules if isinstance(rules, list) else [rules]
        coverage_ = coverage_matrix(rules_, patterns, target, ids=ids)

        return coverage_

    def coverage_size(self, rule, x, ids=None):
        &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

        Args:
            rule (Rule): The rule.
            x (numpy.array): The validation set.
            ids (numpy.array): Unique identifiers to tell each element in `x` apart.
        Returns:
            numpy.array: Number of records of X covered by rule.
        &#34;&#34;&#34;
        return coverage_size(rule, x)

    def binary_fidelity(self, unit, x, y, default=np.nan, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            unit (Unit): The unit to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
            default (int): Default prediction when no rule covers a record.
        Returns:
              (float): The unit&#39;s fidelity_weight
        &#34;&#34;&#34;
        if self.oracle is not None or y is None:
            y = self.oracle.predict(x).round().squeeze()

        return binary_fidelity(unit, x, y, self, default=default, ids=ids)

    def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            units (Union(list, set): The units to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            k (int): Number of rules to use in the Laplacian prediction schema.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
              numpy.array: The units fidelity_weight.
        &#34;&#34;&#34;
        if self.oracle is not None:
            y = (self.oracle.predict(x).round().squeeze())

        scores = np.array([self.binary_fidelity(rule, x, y, default=default) for rule in units])
        coverage = self.coverage(units, x, y)

        predictions = []
        for record in range(len(x)):
            companions = scores[coverage[:, record]]
            companion_units = units[coverage[:, record]]
            top_companions = np.argsort(companions)[-k:]
            top_units = companion_units[top_companions]
            top_fidelities = companions[top_companions]
            top_fidelities_0 = [top_fidelity for top_fidelity, top_unit in zip(top_fidelities, top_units)
                                if top_unit.consequence == 0]
            top_fidelities_1 = [top_fidelity for top_fidelity, top_unit in zip(top_fidelities, top_units)
                                if top_unit.consequence == 1]

            if len(top_fidelities_0) == 0 and len(top_fidelities_1) &gt; 0:
                prediction = 1
            elif len(top_fidelities_1) == 0 and len(top_fidelities_0) &gt; 0:
                prediction = 0
            elif len(top_fidelities_1) == 0 and len(top_fidelities_0) == 0:
                prediction = default
            else:
                prediction = 0 if np.mean(top_fidelities_0) &gt; np.mean(top_fidelities_1) else 1

            predictions.append(prediction)
        predictions = np.array(predictions)
        fidelity = 1 - hamming(predictions, y) if len(y) &gt; 0 else 0

        return fidelity</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="evaluators.Evaluator" href="#evaluators.Evaluator">Evaluator</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="evaluators.DummyEvaluator.binary_fidelity"><code class="name flex">
<span>def <span class="ident">binary_fidelity</span></span>(<span>self, unit, x, y, default=nan, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the goodness of unit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unit</code></strong> :&ensp;<code>Unit</code></dt>
<dd>The unit to evaluate.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @patterns apart.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>int</code></dt>
<dd>Default prediction when no rule covers a record.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(float): The unit's fidelity_weight</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def binary_fidelity(self, unit, x, y, default=np.nan, ids=None):
    &#34;&#34;&#34;Evaluate the goodness of unit.
    Args:
        unit (Unit): The unit to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        default (int): Default prediction when no rule covers a record.
    Returns:
          (float): The unit&#39;s fidelity_weight
    &#34;&#34;&#34;
    if self.oracle is not None or y is None:
        y = self.oracle.predict(x).round().squeeze()

    return binary_fidelity(unit, x, y, self, default=default, ids=ids)</code></pre>
</details>
</dd>
<dt id="evaluators.DummyEvaluator.binary_fidelity_model"><code class="name flex">
<span>def <span class="ident">binary_fidelity_model</span></span>(<span>self, units, x, y, k=1, default=None, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the goodness of unit.</p>
<h2 id="args">Args</h2>
<dl>
<dt>units (Union(list, set): The units to evaluate.</dt>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of rules to use in the Laplacian prediction schema.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>int</code></dt>
<dd>Default prediction for records not covered by the unit.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @patterns apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>The units fidelity_weight.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
    &#34;&#34;&#34;Evaluate the goodness of unit.
    Args:
        units (Union(list, set): The units to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        k (int): Number of rules to use in the Laplacian prediction schema.
        default (int): Default prediction for records not covered by the unit.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
    Returns:
          numpy.array: The units fidelity_weight.
    &#34;&#34;&#34;
    if self.oracle is not None:
        y = (self.oracle.predict(x).round().squeeze())

    scores = np.array([self.binary_fidelity(rule, x, y, default=default) for rule in units])
    coverage = self.coverage(units, x, y)

    predictions = []
    for record in range(len(x)):
        companions = scores[coverage[:, record]]
        companion_units = units[coverage[:, record]]
        top_companions = np.argsort(companions)[-k:]
        top_units = companion_units[top_companions]
        top_fidelities = companions[top_companions]
        top_fidelities_0 = [top_fidelity for top_fidelity, top_unit in zip(top_fidelities, top_units)
                            if top_unit.consequence == 0]
        top_fidelities_1 = [top_fidelity for top_fidelity, top_unit in zip(top_fidelities, top_units)
                            if top_unit.consequence == 1]

        if len(top_fidelities_0) == 0 and len(top_fidelities_1) &gt; 0:
            prediction = 1
        elif len(top_fidelities_1) == 0 and len(top_fidelities_0) &gt; 0:
            prediction = 0
        elif len(top_fidelities_1) == 0 and len(top_fidelities_0) == 0:
            prediction = default
        else:
            prediction = 0 if np.mean(top_fidelities_0) &gt; np.mean(top_fidelities_1) else 1

        predictions.append(prediction)
    predictions = np.array(predictions)
    fidelity = 1 - hamming(predictions, y) if len(y) &gt; 0 else 0

    return fidelity</code></pre>
</details>
</dd>
<dt id="evaluators.DummyEvaluator.coverage"><code class="name flex">
<span>def <span class="ident">coverage</span></span>(<span>self, rules, patterns, target=None, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the coverage of @rules over @patterns.</p>
<h2 id="args">Args</h2>
<dl>
<dt>rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.</dt>
<dt><strong><code>patterns</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The validation set.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels, if any. None otherwise. Defaults to None.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @patterns apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>The coverage matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coverage(self, rules, patterns, target=None, ids=None):
    &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
    Args:
        rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.
        patterns (numpy.array): The validation set.
        target (numpy.array): The labels, if any. None otherwise. Defaults to None.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
    Returns:
        numpy.array: The coverage matrix.
    &#34;&#34;&#34;
    rules_ = rules if isinstance(rules, list) else [rules]
    coverage_ = coverage_matrix(rules_, patterns, target, ids=ids)

    return coverage_</code></pre>
</details>
</dd>
<dt id="evaluators.DummyEvaluator.coverage_size"><code class="name flex">
<span>def <span class="ident">coverage_size</span></span>(<span>self, rule, x, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the cardinality of the coverage of unit on c.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rule</code></strong> :&ensp;<code>Rule</code></dt>
<dd>The rule.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The validation set.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in <code>x</code> apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>Number of records of X covered by rule.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coverage_size(self, rule, x, ids=None):
    &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

    Args:
        rule (Rule): The rule.
        x (numpy.array): The validation set.
        ids (numpy.array): Unique identifiers to tell each element in `x` apart.
    Returns:
        numpy.array: Number of records of X covered by rule.
    &#34;&#34;&#34;
    return coverage_size(rule, x)</code></pre>
</details>
</dd>
<dt id="evaluators.DummyEvaluator.covers"><code class="name flex">
<span>def <span class="ident">covers</span></span>(<span>self, rule, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Does <code>rule</code> cover <code>x</code>?</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rule</code></strong> :&ensp;<code>Rule</code></dt>
<dd>The rule.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The record.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if this rule covers c, False otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def covers(self, rule, x):
    &#34;&#34;&#34;Does `rule` cover `x`?

    Args:
        rule (Rule): The rule.
        x (numpy.array): The record.
    Returns:
        bool: True if this rule covers c, False otherwise.
    &#34;&#34;&#34;
    return covers(rule, x)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="evaluators.Evaluator" href="#evaluators.Evaluator">Evaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="evaluators.Evaluator.bic" href="#evaluators.Evaluator.bic">bic</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="evaluators.Evaluator"><code class="flex name class">
<span>class <span class="ident">Evaluator</span></span>
</code></dt>
<dd>
<div class="desc"><p>Evaluator interface. Evaluator objects provide coverage and fidelity_weight utilities.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Evaluator:
    &#34;&#34;&#34;Evaluator interface. Evaluator objects provide coverage and fidelity_weight utilities.&#34;&#34;&#34;

    @abstractmethod
    def coverage(self, rules, patterns, target=None, ids=None):
        &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
        Args:
            rules (list) or (Rule):
            patterns (numpy.array): The validation set.
            target (numpy.array): The labels, if any. None otherwise. Defaults to None.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
            numpy.array: The coverage matrix.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def coverage_size(self, rule, x, ids=None):
        &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

        Args:
            rule (Rule): The rule.
            x (numpy.array): The validation set.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        Returns:
            int: Number of records of X covered by rule.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def binary_fidelity(self, unit, x, y, ids=None, default=np.nan):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            unit (Unit): The unit to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
            default (int): Default prediction when no rule covers a record.
        Returns:
              float: The unit&#39;s fidelity_weight
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of the `units`.
        Args:
            units (Union(list, set)): The units to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            k (int): Number of rules to use in the Laplacian prediction schema.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): Unique identifiers to tell each element in @c apart.
        Returns:
            float: The units fidelity_weight.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def covers(self, rule, x):
        &#34;&#34;&#34;Does @rule cover c?

        Args:
            rule (Rule): The rule.
            x (numpy.array): The record.
        Returns:
            bool: True if this rule covers c, False otherwise.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
        &#34;&#34;&#34;
        Compute the Bayesian Information Criterion for the given `rules` set.
        Args:
            rules (set): Ruleset.
            vl (numpy.array): Validation set.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
            complexity_weight (float): Weight to complexity_weight (BIC-wise).
        Returns:
            tuple: Triple (BIC, log likelihood, complexity_weight).
        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="evaluators.DummyEvaluator" href="#evaluators.DummyEvaluator">DummyEvaluator</a></li>
<li><a title="evaluators.MemEvaluator" href="#evaluators.MemEvaluator">MemEvaluator</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="evaluators.Evaluator.bic"><code class="name flex">
<span>def <span class="ident">bic</span></span>(<span>self, rules, vl, fidelity_weight=1.0, complexity_weight=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Bayesian Information Criterion for the given <code>rules</code> set.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rules</code></strong> :&ensp;<code>set</code></dt>
<dd>Ruleset.</dd>
<dt><strong><code>vl</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Validation set.</dd>
<dt><strong><code>fidelity_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight to fidelity_weight (BIC-wise).</dd>
<dt><strong><code>complexity_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight to complexity_weight (BIC-wise).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Triple (BIC, log likelihood, complexity_weight).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
    &#34;&#34;&#34;
    Compute the Bayesian Information Criterion for the given `rules` set.
    Args:
        rules (set): Ruleset.
        vl (numpy.array): Validation set.
        fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
        complexity_weight (float): Weight to complexity_weight (BIC-wise).
    Returns:
        tuple: Triple (BIC, log likelihood, complexity_weight).
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="evaluators.Evaluator.binary_fidelity"><code class="name flex">
<span>def <span class="ident">binary_fidelity</span></span>(<span>self, unit, x, y, ids=None, default=nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the goodness of unit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unit</code></strong> :&ensp;<code>Unit</code></dt>
<dd>The unit to evaluate.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @patterns apart.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>int</code></dt>
<dd>Default prediction when no rule covers a record.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The unit's fidelity_weight</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def binary_fidelity(self, unit, x, y, ids=None, default=np.nan):
    &#34;&#34;&#34;Evaluate the goodness of unit.
    Args:
        unit (Unit): The unit to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
        default (int): Default prediction when no rule covers a record.
    Returns:
          float: The unit&#39;s fidelity_weight
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="evaluators.Evaluator.binary_fidelity_model"><code class="name flex">
<span>def <span class="ident">binary_fidelity_model</span></span>(<span>self, units, x, y, k=1, default=None, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the goodness of the <code>units</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt>units (Union(list, set)): The units to evaluate.</dt>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of rules to use in the Laplacian prediction schema.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>int</code></dt>
<dd>Default prediction for records not covered by the unit.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @c apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The units fidelity_weight.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
    &#34;&#34;&#34;Evaluate the goodness of the `units`.
    Args:
        units (Union(list, set)): The units to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        k (int): Number of rules to use in the Laplacian prediction schema.
        default (int): Default prediction for records not covered by the unit.
        ids (numpy.array): Unique identifiers to tell each element in @c apart.
    Returns:
        float: The units fidelity_weight.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="evaluators.Evaluator.coverage"><code class="name flex">
<span>def <span class="ident">coverage</span></span>(<span>self, rules, patterns, target=None, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the coverage of @rules over @patterns.</p>
<h2 id="args">Args</h2>
<dl>
<dt>rules (list) or (Rule):</dt>
<dt><strong><code>patterns</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The validation set.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels, if any. None otherwise. Defaults to None.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @patterns apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>The coverage matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def coverage(self, rules, patterns, target=None, ids=None):
    &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
    Args:
        rules (list) or (Rule):
        patterns (numpy.array): The validation set.
        target (numpy.array): The labels, if any. None otherwise. Defaults to None.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
    Returns:
        numpy.array: The coverage matrix.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="evaluators.Evaluator.coverage_size"><code class="name flex">
<span>def <span class="ident">coverage_size</span></span>(<span>self, rule, x, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the cardinality of the coverage of unit on c.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rule</code></strong> :&ensp;<code>Rule</code></dt>
<dd>The rule.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The validation set.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @patterns apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of records of X covered by rule.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def coverage_size(self, rule, x, ids=None):
    &#34;&#34;&#34;Evaluate the cardinality of the coverage of unit on c.

    Args:
        rule (Rule): The rule.
        x (numpy.array): The validation set.
        ids (numpy.array): Unique identifiers to tell each element in @patterns apart.
    Returns:
        int: Number of records of X covered by rule.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="evaluators.Evaluator.covers"><code class="name flex">
<span>def <span class="ident">covers</span></span>(<span>self, rule, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Does @rule cover c?</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rule</code></strong> :&ensp;<code>Rule</code></dt>
<dd>The rule.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The record.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if this rule covers c, False otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def covers(self, rule, x):
    &#34;&#34;&#34;Does @rule cover c?

    Args:
        rule (Rule): The rule.
        x (numpy.array): The record.
    Returns:
        bool: True if this rule covers c, False otherwise.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="evaluators.MemEvaluator"><code class="flex name class">
<span>class <span class="ident">MemEvaluator</span></span>
<span>(</span><span>oracle)</span>
</code></dt>
<dd>
<div class="desc"><p>Memoization-aware Evaluator to avoid evaluating the same measures over the same data.</p>
<p>Constructor.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MemEvaluator(Evaluator):
    &#34;&#34;&#34;Memoization-aware Evaluator to avoid evaluating the same measures over the same data.&#34;&#34;&#34;

    def __init__(self, oracle):
        &#34;&#34;&#34;Constructor.&#34;&#34;&#34;
        self.oracle = oracle
        self.coverages = dict()
        self.perfect_coverages = dict()
        self.intersecting = dict()
        self.bics = dict()
        self.distances = dict()
        self.binary_fidelities = dict()
        self.coverage_sizes = dict()
        self.scores = dict()

    @abstractmethod
    def covers(self, rule, x):
        &#34;&#34;&#34;Does @rule cover c?

        Args:
            rule (Rule): The rule.
            x (numpy.array): The record.
        Returns:
            bool: True if this rule covers c, False otherwise.
        &#34;&#34;&#34;
        return covers(rule, x)

    def coverage(self, rules, patterns, targets=None, ids=None):
        &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
        Args:
            rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.
            patterns (numpy.array): The validation set.
            targets (numpy.array): The labels, if any. None otherwise. Defaults to None.
            ids (numpy.array): IDS of the given `patterns`, used to speed up evaluation.
        Returns:
            numpy.array: The coverage matrix.
        &#34;&#34;&#34;
        rules_ = [rules] if not isinstance(rules, list) and not isinstance(rules, set) else rules

        if targets is None:
            for rule in rules_:
                if rule not in self.coverages:
                    self.coverages[rule] = coverage_matrix(rule, patterns, targets)
            cov = np.array([self.coverages[rule] for rule in rules_])
        else:
            for rule in rules_:
                if rule not in self.perfect_coverages:
                    self.perfect_coverages[rule] = coverage_matrix(rule, patterns, targets)
            cov = np.array([self.perfect_coverages[rule] for rule in rules_])

        cov = cov[:, ids] if ids is not None else cov

        return cov

    def distance(self, A, B, x, ids=None):
        &#34;&#34;&#34;
        Compute the distance between ruleset `A` and ruleset `B`.
        Args:
            A (iterable): Ruleset.
            B (iterable): Ruleset.
            x (numpy.array): Data.
            ids (numpy.array): IDS of the given `x`, used to speed up evaluation.
        Returns:
            (float): The Jaccard distance between the two.
        &#34;&#34;&#34;
        if tuple(A) in self.distances and tuple(B) in self.distances[tuple(A)]:
            diff = self.distances[tuple(A)][tuple(B)]
            return diff
        if tuple(B) in self.distances and tuple(A) in self.distances[tuple(B)]:
            diff = self.distances[tuple(B)][tuple(A)]
            return diff

        # New distance to compute
        coverage_A, coverage_B = self.coverage(A, x, ids=ids).sum(axis=0), self.coverage(B, x, ids=ids).sum(axis=0)
        diff = hamming(coverage_A, coverage_B)
        if tuple(A) in self.distances:
            self.distances[tuple(A)][tuple(B)] = diff
        if tuple(B) in self.distances:
            self.distances[tuple(B)][tuple(A)] = diff

        # First time for A
        if tuple(A) not in self.distances:
            self.distances[tuple(A)] = {tuple(B): diff}
        # First time for B
        if tuple(B) not in self.distances:
            self.distances[tuple(B)] = {tuple(A): diff}

        return diff

    def binary_fidelity(self, unit, x, y, default=np.nan, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of unit.
        Args:
            unit (Unit): The unit to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): IDS of the given `x`, used to speed up evaluation.
        Returns:
              float: The unit&#39;s fidelity_weight
        &#34;&#34;&#34;
        if y is None:
            y = self.oracle.predict(x).round().squeeze()

        if ids is None:
            self.binary_fidelities[unit] = self.binary_fidelities.get(unit, binary_fidelity(unit, x, y, self,
                                                                                            default=default, ids=None))
            fidelity = self.binary_fidelities[unit]
        else:
            fidelity = binary_fidelity(unit, x, y, self, default=default, ids=ids)

        return fidelity

    def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
        &#34;&#34;&#34;Evaluate the goodness of the `units`.
        Args:
            units (Union(list, set)): The units to evaluate.
            x (numpy.array): The data.
            y (numpy.array): The labels.
            k (int): Number of rules to use in the Laplacian prediction schema.
            default (int): Default prediction for records not covered by the unit.
            ids (numpy.array): Unique identifiers to tell each element in @c apart.
        Returns:
              float: The units fidelity_weight.
        &#34;&#34;&#34;
        if y is None:
            y = self.oracle.predict(x).squeeze().round()

        scores = np.array([self.binary_fidelity(rule, x, y, default=default) for rule in units])
        coverage = self.coverage(units, x)

        if len(units) == 0:
            predictions = [default] * y.shape[0]
        else:
            rules_consequences = np.array([r.consequence for r in units])
            # Fast computation for k = 1
            if k == 1:
                weighted_coverage_scores = coverage * scores.reshape(-1, 1)  # Coverage matrix weighted by score
                # Best score per row (i.e., record)
                best_rule_per_record_idx = weighted_coverage_scores.argmax(axis=0).squeeze()
                predictions = rules_consequences[best_rule_per_record_idx]
                # Replace predictions of non-covered records w/ default prediction
                predictions[coverage.sum(axis=0) == 0] = default
            # Iterative computation
            else:
                predictions = []
                for record in range(len(x)):
                    record_coverage = np.argwhere(coverage[:, record]).ravel()
                    if len(record_coverage) == 0:
                        prediction = default
                    else:
                        companions_0 = record_coverage[rules_consequences[record_coverage] == 0]
                        companions_1 = record_coverage[rules_consequences[record_coverage] == 1]
                        scores_0 = scores[companions_0]
                        scores_1 = scores[companions_1]
                        np.argsort_scores_0 = np.flip(np.argsort(scores[companions_0])[-k:])
                        np.argsort_scores_1 = np.flip(np.argsort(scores[companions_1])[-k:])
                        top_scores_0 = scores_0[np.argsort_scores_0]
                        top_scores_1 = scores_1[np.argsort_scores_1]

                        if len(top_scores_0) == 0 and len(top_scores_1) &gt; 0:
                            prediction = 1
                        elif len(top_scores_1) == 0 and len(top_scores_0) &gt; 0:
                            prediction = 0
                        elif len(top_scores_1) == 0 and len(top_scores_0) == 0:
                            prediction = default
                        else:
                            prediction = 0 if np.mean(top_scores_0) &gt; np.mean(top_scores_1) else 1

                    predictions.append(prediction)
                predictions = np.array(predictions)
        fidelity = 1 - hamming(predictions, y) if len(y) &gt; 0 else 0

        return fidelity

    def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
        &#34;&#34;&#34;
        Compute the Bayesian Information Criterion for the given `rules` set.
        Args:
            rules (set): Ruleset.
            vl (numpy.array): Validation set.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
            complexity_weight (float): Weight to complexity_weight (BIC-wise).
        Returns:
            float: Model BIC 
        &#34;&#34;&#34;
        if tuple(rules) in self.bics:
            model_bic = self.bics[tuple(rules)]
        else:
            x, y = vl[:, :-1], vl[:, -1]
            n, m = x.shape
            default = int(y.mean().round())
            log_likelihood = self.binary_fidelity_model(rules, x, y, default=default)

            model_complexity = np.mean([len(r) / m for r in rules])
            model_bic = - (fidelity_weight * log_likelihood - complexity_weight * model_complexity / n)

            logger.debug(&#39;Log likelihood: &#39; + str(log_likelihood) + &#39; | Complexity: &#39; + str(model_complexity))

            self.bics[tuple(rules)] = model_bic

        return model_bic

    def forget(self, rules, A=None, B=None):
        &#34;&#34;&#34;
        Remove rules from this Evaluator&#39;s memory. Return the updated evaluator.
        Args:
            rules (iterable): Rules to remove.
            A (set): Rules merged.
            B (set): Rules merged.
        Returns:
            MemEvaluator: This evaluator with no memory of `rules`.

        &#34;&#34;&#34;
        for rule in rules:
            if rule in self.binary_fidelities:
                del self.binary_fidelities[rule]
            if rule in self.coverages:
                del self.coverages[rule]
            if rule in self.coverage_sizes:
                del self.coverage_sizes[rule]
            if rule in self.perfect_coverages:
                del self.perfect_coverages[rule]
            if rule in self.scores:
                del self.scores[rule]

        if A is not None and B is not None:
            # Delete the whole A, as it has been merged and does not exist anymore
            del self.distances[tuple(A)]
            # Delete the whole B, as it has been merged and does not exist anymore
            del self.distances[tuple(B)]
            # Delete every reference to any of them, as they have been merged and do not exist anymore
            for T in self.distances:
                if tuple(A) in self.distances[T]:
                    del self.distances[T][tuple(A)]
                if tuple(B) in self.distances[T]:
                    del self.distances[T][tuple(B)]

        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="evaluators.Evaluator" href="#evaluators.Evaluator">Evaluator</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="evaluators.MemEvaluator.bic"><code class="name flex">
<span>def <span class="ident">bic</span></span>(<span>self, rules, vl, fidelity_weight=1.0, complexity_weight=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Bayesian Information Criterion for the given <code>rules</code> set.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rules</code></strong> :&ensp;<code>set</code></dt>
<dd>Ruleset.</dd>
<dt><strong><code>vl</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Validation set.</dd>
<dt><strong><code>fidelity_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight to fidelity_weight (BIC-wise).</dd>
<dt><strong><code>complexity_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight to complexity_weight (BIC-wise).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Model BIC</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bic(self, rules, vl, fidelity_weight=1., complexity_weight=1.):
    &#34;&#34;&#34;
    Compute the Bayesian Information Criterion for the given `rules` set.
    Args:
        rules (set): Ruleset.
        vl (numpy.array): Validation set.
        fidelity_weight (float): Weight to fidelity_weight (BIC-wise).
        complexity_weight (float): Weight to complexity_weight (BIC-wise).
    Returns:
        float: Model BIC 
    &#34;&#34;&#34;
    if tuple(rules) in self.bics:
        model_bic = self.bics[tuple(rules)]
    else:
        x, y = vl[:, :-1], vl[:, -1]
        n, m = x.shape
        default = int(y.mean().round())
        log_likelihood = self.binary_fidelity_model(rules, x, y, default=default)

        model_complexity = np.mean([len(r) / m for r in rules])
        model_bic = - (fidelity_weight * log_likelihood - complexity_weight * model_complexity / n)

        logger.debug(&#39;Log likelihood: &#39; + str(log_likelihood) + &#39; | Complexity: &#39; + str(model_complexity))

        self.bics[tuple(rules)] = model_bic

    return model_bic</code></pre>
</details>
</dd>
<dt id="evaluators.MemEvaluator.binary_fidelity"><code class="name flex">
<span>def <span class="ident">binary_fidelity</span></span>(<span>self, unit, x, y, default=nan, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the goodness of unit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unit</code></strong> :&ensp;<code>Unit</code></dt>
<dd>The unit to evaluate.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>int</code></dt>
<dd>Default prediction for records not covered by the unit.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>IDS of the given <code>x</code>, used to speed up evaluation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The unit's fidelity_weight</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def binary_fidelity(self, unit, x, y, default=np.nan, ids=None):
    &#34;&#34;&#34;Evaluate the goodness of unit.
    Args:
        unit (Unit): The unit to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        default (int): Default prediction for records not covered by the unit.
        ids (numpy.array): IDS of the given `x`, used to speed up evaluation.
    Returns:
          float: The unit&#39;s fidelity_weight
    &#34;&#34;&#34;
    if y is None:
        y = self.oracle.predict(x).round().squeeze()

    if ids is None:
        self.binary_fidelities[unit] = self.binary_fidelities.get(unit, binary_fidelity(unit, x, y, self,
                                                                                        default=default, ids=None))
        fidelity = self.binary_fidelities[unit]
    else:
        fidelity = binary_fidelity(unit, x, y, self, default=default, ids=ids)

    return fidelity</code></pre>
</details>
</dd>
<dt id="evaluators.MemEvaluator.binary_fidelity_model"><code class="name flex">
<span>def <span class="ident">binary_fidelity_model</span></span>(<span>self, units, x, y, k=1, default=None, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the goodness of the <code>units</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt>units (Union(list, set)): The units to evaluate.</dt>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of rules to use in the Laplacian prediction schema.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>int</code></dt>
<dd>Default prediction for records not covered by the unit.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Unique identifiers to tell each element in @c apart.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The units fidelity_weight.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def binary_fidelity_model(self, units, x, y, k=1, default=None, ids=None):
    &#34;&#34;&#34;Evaluate the goodness of the `units`.
    Args:
        units (Union(list, set)): The units to evaluate.
        x (numpy.array): The data.
        y (numpy.array): The labels.
        k (int): Number of rules to use in the Laplacian prediction schema.
        default (int): Default prediction for records not covered by the unit.
        ids (numpy.array): Unique identifiers to tell each element in @c apart.
    Returns:
          float: The units fidelity_weight.
    &#34;&#34;&#34;
    if y is None:
        y = self.oracle.predict(x).squeeze().round()

    scores = np.array([self.binary_fidelity(rule, x, y, default=default) for rule in units])
    coverage = self.coverage(units, x)

    if len(units) == 0:
        predictions = [default] * y.shape[0]
    else:
        rules_consequences = np.array([r.consequence for r in units])
        # Fast computation for k = 1
        if k == 1:
            weighted_coverage_scores = coverage * scores.reshape(-1, 1)  # Coverage matrix weighted by score
            # Best score per row (i.e., record)
            best_rule_per_record_idx = weighted_coverage_scores.argmax(axis=0).squeeze()
            predictions = rules_consequences[best_rule_per_record_idx]
            # Replace predictions of non-covered records w/ default prediction
            predictions[coverage.sum(axis=0) == 0] = default
        # Iterative computation
        else:
            predictions = []
            for record in range(len(x)):
                record_coverage = np.argwhere(coverage[:, record]).ravel()
                if len(record_coverage) == 0:
                    prediction = default
                else:
                    companions_0 = record_coverage[rules_consequences[record_coverage] == 0]
                    companions_1 = record_coverage[rules_consequences[record_coverage] == 1]
                    scores_0 = scores[companions_0]
                    scores_1 = scores[companions_1]
                    np.argsort_scores_0 = np.flip(np.argsort(scores[companions_0])[-k:])
                    np.argsort_scores_1 = np.flip(np.argsort(scores[companions_1])[-k:])
                    top_scores_0 = scores_0[np.argsort_scores_0]
                    top_scores_1 = scores_1[np.argsort_scores_1]

                    if len(top_scores_0) == 0 and len(top_scores_1) &gt; 0:
                        prediction = 1
                    elif len(top_scores_1) == 0 and len(top_scores_0) &gt; 0:
                        prediction = 0
                    elif len(top_scores_1) == 0 and len(top_scores_0) == 0:
                        prediction = default
                    else:
                        prediction = 0 if np.mean(top_scores_0) &gt; np.mean(top_scores_1) else 1

                predictions.append(prediction)
            predictions = np.array(predictions)
    fidelity = 1 - hamming(predictions, y) if len(y) &gt; 0 else 0

    return fidelity</code></pre>
</details>
</dd>
<dt id="evaluators.MemEvaluator.coverage"><code class="name flex">
<span>def <span class="ident">coverage</span></span>(<span>self, rules, patterns, targets=None, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the coverage of @rules over @patterns.</p>
<h2 id="args">Args</h2>
<dl>
<dt>rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.</dt>
<dt><strong><code>patterns</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The validation set.</dd>
<dt><strong><code>targets</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>The labels, if any. None otherwise. Defaults to None.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>IDS of the given <code>patterns</code>, used to speed up evaluation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>The coverage matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coverage(self, rules, patterns, targets=None, ids=None):
    &#34;&#34;&#34;Compute the coverage of @rules over @patterns.
    Args:
        rules (Union(Rule, list): Rule (or list of rules) whose coverage to compute.
        patterns (numpy.array): The validation set.
        targets (numpy.array): The labels, if any. None otherwise. Defaults to None.
        ids (numpy.array): IDS of the given `patterns`, used to speed up evaluation.
    Returns:
        numpy.array: The coverage matrix.
    &#34;&#34;&#34;
    rules_ = [rules] if not isinstance(rules, list) and not isinstance(rules, set) else rules

    if targets is None:
        for rule in rules_:
            if rule not in self.coverages:
                self.coverages[rule] = coverage_matrix(rule, patterns, targets)
        cov = np.array([self.coverages[rule] for rule in rules_])
    else:
        for rule in rules_:
            if rule not in self.perfect_coverages:
                self.perfect_coverages[rule] = coverage_matrix(rule, patterns, targets)
        cov = np.array([self.perfect_coverages[rule] for rule in rules_])

    cov = cov[:, ids] if ids is not None else cov

    return cov</code></pre>
</details>
</dd>
<dt id="evaluators.MemEvaluator.distance"><code class="name flex">
<span>def <span class="ident">distance</span></span>(<span>self, A, B, x, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the distance between ruleset <code>A</code> and ruleset <code>B</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Ruleset.</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Ruleset.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Data.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>IDS of the given <code>x</code>, used to speed up evaluation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(float): The Jaccard distance between the two.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distance(self, A, B, x, ids=None):
    &#34;&#34;&#34;
    Compute the distance between ruleset `A` and ruleset `B`.
    Args:
        A (iterable): Ruleset.
        B (iterable): Ruleset.
        x (numpy.array): Data.
        ids (numpy.array): IDS of the given `x`, used to speed up evaluation.
    Returns:
        (float): The Jaccard distance between the two.
    &#34;&#34;&#34;
    if tuple(A) in self.distances and tuple(B) in self.distances[tuple(A)]:
        diff = self.distances[tuple(A)][tuple(B)]
        return diff
    if tuple(B) in self.distances and tuple(A) in self.distances[tuple(B)]:
        diff = self.distances[tuple(B)][tuple(A)]
        return diff

    # New distance to compute
    coverage_A, coverage_B = self.coverage(A, x, ids=ids).sum(axis=0), self.coverage(B, x, ids=ids).sum(axis=0)
    diff = hamming(coverage_A, coverage_B)
    if tuple(A) in self.distances:
        self.distances[tuple(A)][tuple(B)] = diff
    if tuple(B) in self.distances:
        self.distances[tuple(B)][tuple(A)] = diff

    # First time for A
    if tuple(A) not in self.distances:
        self.distances[tuple(A)] = {tuple(B): diff}
    # First time for B
    if tuple(B) not in self.distances:
        self.distances[tuple(B)] = {tuple(A): diff}

    return diff</code></pre>
</details>
</dd>
<dt id="evaluators.MemEvaluator.forget"><code class="name flex">
<span>def <span class="ident">forget</span></span>(<span>self, rules, A=None, B=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove rules from this Evaluator's memory. Return the updated evaluator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rules</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Rules to remove.</dd>
<dt><strong><code>A</code></strong> :&ensp;<code>set</code></dt>
<dd>Rules merged.</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>set</code></dt>
<dd>Rules merged.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="evaluators.MemEvaluator" href="#evaluators.MemEvaluator">MemEvaluator</a></code></dt>
<dd>This evaluator with no memory of <code>rules</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forget(self, rules, A=None, B=None):
    &#34;&#34;&#34;
    Remove rules from this Evaluator&#39;s memory. Return the updated evaluator.
    Args:
        rules (iterable): Rules to remove.
        A (set): Rules merged.
        B (set): Rules merged.
    Returns:
        MemEvaluator: This evaluator with no memory of `rules`.

    &#34;&#34;&#34;
    for rule in rules:
        if rule in self.binary_fidelities:
            del self.binary_fidelities[rule]
        if rule in self.coverages:
            del self.coverages[rule]
        if rule in self.coverage_sizes:
            del self.coverage_sizes[rule]
        if rule in self.perfect_coverages:
            del self.perfect_coverages[rule]
        if rule in self.scores:
            del self.scores[rule]

    if A is not None and B is not None:
        # Delete the whole A, as it has been merged and does not exist anymore
        del self.distances[tuple(A)]
        # Delete the whole B, as it has been merged and does not exist anymore
        del self.distances[tuple(B)]
        # Delete every reference to any of them, as they have been merged and do not exist anymore
        for T in self.distances:
            if tuple(A) in self.distances[T]:
                del self.distances[T][tuple(A)]
            if tuple(B) in self.distances[T]:
                del self.distances[T][tuple(B)]

    return self</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="evaluators.Evaluator" href="#evaluators.Evaluator">Evaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="evaluators.Evaluator.coverage_size" href="#evaluators.Evaluator.coverage_size">coverage_size</a></code></li>
<li><code><a title="evaluators.Evaluator.covers" href="#evaluators.Evaluator.covers">covers</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="evaluators.binary_fidelity" href="#evaluators.binary_fidelity">binary_fidelity</a></code></li>
<li><code><a title="evaluators.coverage_matrix" href="#evaluators.coverage_matrix">coverage_matrix</a></code></li>
<li><code><a title="evaluators.coverage_size" href="#evaluators.coverage_size">coverage_size</a></code></li>
<li><code><a title="evaluators.covers" href="#evaluators.covers">covers</a></code></li>
<li><code><a title="evaluators.validate" href="#evaluators.validate">validate</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="evaluators.DummyEvaluator" href="#evaluators.DummyEvaluator">DummyEvaluator</a></code></h4>
<ul class="">
<li><code><a title="evaluators.DummyEvaluator.binary_fidelity" href="#evaluators.DummyEvaluator.binary_fidelity">binary_fidelity</a></code></li>
<li><code><a title="evaluators.DummyEvaluator.binary_fidelity_model" href="#evaluators.DummyEvaluator.binary_fidelity_model">binary_fidelity_model</a></code></li>
<li><code><a title="evaluators.DummyEvaluator.coverage" href="#evaluators.DummyEvaluator.coverage">coverage</a></code></li>
<li><code><a title="evaluators.DummyEvaluator.coverage_size" href="#evaluators.DummyEvaluator.coverage_size">coverage_size</a></code></li>
<li><code><a title="evaluators.DummyEvaluator.covers" href="#evaluators.DummyEvaluator.covers">covers</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="evaluators.Evaluator" href="#evaluators.Evaluator">Evaluator</a></code></h4>
<ul class="">
<li><code><a title="evaluators.Evaluator.bic" href="#evaluators.Evaluator.bic">bic</a></code></li>
<li><code><a title="evaluators.Evaluator.binary_fidelity" href="#evaluators.Evaluator.binary_fidelity">binary_fidelity</a></code></li>
<li><code><a title="evaluators.Evaluator.binary_fidelity_model" href="#evaluators.Evaluator.binary_fidelity_model">binary_fidelity_model</a></code></li>
<li><code><a title="evaluators.Evaluator.coverage" href="#evaluators.Evaluator.coverage">coverage</a></code></li>
<li><code><a title="evaluators.Evaluator.coverage_size" href="#evaluators.Evaluator.coverage_size">coverage_size</a></code></li>
<li><code><a title="evaluators.Evaluator.covers" href="#evaluators.Evaluator.covers">covers</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="evaluators.MemEvaluator" href="#evaluators.MemEvaluator">MemEvaluator</a></code></h4>
<ul class="">
<li><code><a title="evaluators.MemEvaluator.bic" href="#evaluators.MemEvaluator.bic">bic</a></code></li>
<li><code><a title="evaluators.MemEvaluator.binary_fidelity" href="#evaluators.MemEvaluator.binary_fidelity">binary_fidelity</a></code></li>
<li><code><a title="evaluators.MemEvaluator.binary_fidelity_model" href="#evaluators.MemEvaluator.binary_fidelity_model">binary_fidelity_model</a></code></li>
<li><code><a title="evaluators.MemEvaluator.coverage" href="#evaluators.MemEvaluator.coverage">coverage</a></code></li>
<li><code><a title="evaluators.MemEvaluator.distance" href="#evaluators.MemEvaluator.distance">distance</a></code></li>
<li><code><a title="evaluators.MemEvaluator.forget" href="#evaluators.MemEvaluator.forget">forget</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>