{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc3a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from metadata.meta import CheckPoint\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from trepan_generation import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import six\n",
    "import sys\n",
    "import pydot \n",
    "import pydotplus \n",
    "from IPython.display import Image  \n",
    "import graphviz\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from abstract import *\n",
    "from sklearn import mixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3706ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['temp_hand',\n",
    " 'acceleration_16_x_hand',\n",
    " 'acceleration_16_y_hand',\n",
    " 'acceleration_16_z_hand',\n",
    " 'acceleration_6_x_hand',\n",
    " 'acceleration_6_y_hand',\n",
    " 'acceleration_6_z_hand',\n",
    " 'gyroscope_x_hand',\n",
    " 'gyroscope_y_hand',\n",
    " 'gyroscope_z_hand',\n",
    " 'magnetometer_x_hand',\n",
    " 'magnetometer_y_hand',\n",
    " 'magnetometer_z_hand',\n",
    " 'temp_chest',\n",
    " 'acceleration_16_x_chest',\n",
    " 'acceleration_16_y_chest',\n",
    " 'acceleration_16_z_chest',\n",
    " 'acceleration_6_x_chest',\n",
    " 'acceleration_6_y_chest',\n",
    " 'acceleration_6_z_chest',\n",
    " 'gyroscope_x_chest',\n",
    " 'gyroscope_y_chest',\n",
    " 'gyroscope_z_chest',\n",
    " 'magnetometer_x_chest',\n",
    " 'magnetometer_y_chest',\n",
    " 'magnetometer_z_chest',\n",
    " 'temp_ankle',\n",
    " 'acceleration_16_x_ankle',\n",
    " 'acceleration_16_y_ankle',\n",
    " 'acceleration_16_z_ankle',\n",
    " 'acceleration_6_x_ankle',\n",
    " 'acceleration_6_y_ankle',\n",
    " 'acceleration_6_z_ankle',\n",
    " 'gyroscope_x_ankle',\n",
    " 'gyroscope_y_ankle',\n",
    " 'gyroscope_z_ankle',\n",
    " 'magnetometer_x_ankle',\n",
    " 'magnetometer_y_ankle',\n",
    " 'magnetometer_z_ankle']\n",
    "class_name  = ['motion_n', 'motion_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170a6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = ['age', 'workclass', 'educational-num', 'marital-status', 'occupation',\n",
    "#         'relationship', 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "#         'native-country']\n",
    "# class_name = [\"<=50K\",\">50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb0fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## real data\n",
    "\n",
    "# train0= pd.read_csv('./examples/datasets/node_0/adult_train_enc.csv')\n",
    "# test0 = pd.read_csv('./examples/datasets/node_0/adult_test_enc.csv')\n",
    "# train1= pd.read_csv('./examples/datasets/node_1/adult_train_enc.csv')\n",
    "# test1 = pd.read_csv('./examples/datasets/node_1/adult_test_enc.csv')\n",
    "# train2= pd.read_csv('./examples/datasets/node_2/adult_train_enc.csv')\n",
    "# test2 = pd.read_csv('./examples/datasets/node_2/adult_test_enc.csv')\n",
    "# train3= pd.read_csv('./examples/datasets/node_3/adult_train_enc.csv')\n",
    "# test3 = pd.read_csv('./examples/datasets/node_3/adult_test_enc.csv')\n",
    "\n",
    "# X_train_C0= train0.iloc[:,:12].to_numpy()\n",
    "# y_train_C0 = train0.iloc[:,12:].to_numpy()\n",
    "# X_test_C0= test0.iloc[:,:12].to_numpy()\n",
    "# y_test_C0 = test0.iloc[:,12:].to_numpy()\n",
    "\n",
    "\n",
    "# X_train_C1= train1.iloc[:,:12].to_numpy()\n",
    "# y_train_C1 = train1.iloc[:,12:].to_numpy()\n",
    "# X_test_C1= test1.iloc[:,:12].to_numpy()\n",
    "# y_test_C1 = test1.iloc[:,12:].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# X_train_C2= train2.iloc[:,:12].to_numpy()\n",
    "# y_train_C2 = train2.iloc[:,12:].to_numpy()\n",
    "# X_test_C2= test2.iloc[:,:12].to_numpy()\n",
    "# y_test_C2 = test2.iloc[:,12:].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# X_train_C3= train3.iloc[:,:12].to_numpy()\n",
    "# y_train_C3 = train3.iloc[:,12:].to_numpy()\n",
    "# X_test_C3= test3.iloc[:,:12].to_numpy()\n",
    "# y_test_C3 = test3.iloc[:,12:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da87be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################### generated data\n",
    "\n",
    "train_C0 = pd.read_csv('load the trepan generated data')\n",
    "labels_C0 = pd.read_csv('load the trepan generated labels')\n",
    "train_C0=train_C0.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "labels_C0=labels_C0.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "train_C1 = pd.read_csv('load the trepan generated data')\n",
    "labels_C1 = pd.read_csv('load the trepan generated labels')\n",
    "train_C1=train_C1.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "labels_C1=labels_C1.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "train_C2 = pd.read_csv('load the trepan generated data')\n",
    "labels_C2 = pd.read_csv('load the trepan generated labels')\n",
    "train_C2=train_C2.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "labels_C2=labels_C2.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "train_C3 = pd.read_csv('load the trepan generated data')\n",
    "labels_C3 = pd.read_csv('load the trepan generated labels')\n",
    "train_C3=train_C3.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "labels_C3=labels_C3.drop(['Unnamed: 0'], axis=1).to_numpy()\n",
    "\n",
    "X_train_C0, X_test_C0, y_train_C0, y_test_C0 = train_test_split(train_C0, labels_C0, test_size=0.33, random_state=42)\n",
    "X_train_C1, X_test_C1, y_train_C1, y_test_C1 = train_test_split(train_C1, labels_C1, test_size=0.33, random_state=42)\n",
    "X_train_C2, X_test_C2, y_train_C2, y_test_C2 = train_test_split(train_C2, labels_C2, test_size=0.33, random_state=42)\n",
    "X_train_C3, X_test_C3, y_train_C3, y_test_C3 = train_test_split(train_C3, labels_C3, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f08a323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97738675, 0.71164318, 0.37744782, ..., 0.52007365, 0.62697575,\n",
       "        0.39582267],\n",
       "       [0.80706859, 0.68587454, 0.42709191, ..., 0.60805132, 0.55732578,\n",
       "        0.58994944],\n",
       "       [0.97165296, 0.60612957, 0.32319788, ..., 0.54588904, 0.57238113,\n",
       "        0.47926608],\n",
       "       ...,\n",
       "       [0.65059013, 0.69770668, 0.40032697, ..., 0.49139632, 0.76520501,\n",
       "        0.41159121],\n",
       "       [0.86124224, 0.59441531, 0.41383496, ..., 0.47002444, 0.67569765,\n",
       "        0.6420162 ],\n",
       "       [0.93743707, 0.63465023, 0.39810187, ..., 0.55637161, 0.50529908,\n",
       "        0.5594669 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_C0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe639f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'path to the trained trepan trees'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bfbcd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree =  pickle.load(open(path+'/explainer_lssdpt.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10046b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the trees\n",
    "tree_C0 = pickle.load(open(path+'tree path', 'rb'))\n",
    "tree_C1 = pickle.load(open(path+'tree path', 'rb'))\n",
    "tree_C2 = pickle.load(open(path+'tree path', 'rb'))\n",
    "tree_C3 = pickle.load(open(path+'tree path', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "427562ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n",
      "  Iteration 10\t time lapse 1.56742s\t ll change 0.00785\n",
      "Initialization converged: True\t time lapse 2.61889s\t ll 53.00683\n",
      "Initialization 0\n",
      "  Iteration 10\t time lapse 1.61246s\t ll change 0.00774\n",
      "Initialization converged: True\t time lapse 2.71246s\t ll 53.03279\n",
      "Initialization 0\n",
      "  Iteration 10\t time lapse 1.64850s\t ll change 0.08659\n",
      "  Iteration 20\t time lapse 1.39326s\t ll change 0.00081\n",
      "Initialization converged: True\t time lapse 3.04276s\t ll 56.73858\n",
      "Initialization 0\n",
      "  Iteration 10\t time lapse 1.59144s\t ll change 0.08319\n",
      "  Iteration 20\t time lapse 1.38526s\t ll change 0.00196\n",
      "Initialization converged: True\t time lapse 3.15086s\t ll 56.72823\n"
     ]
    }
   ],
   "source": [
    "# train the GMM models\n",
    "GMM_C0 = mixture.GaussianMixture(n_components=2, covariance_type='full', verbose=3).fit(X_train_C0)\n",
    "GMM_C1 = mixture.GaussianMixture(n_components=2, covariance_type='full', verbose=3).fit(X_train_C1)\n",
    "GMM_C2 = mixture.GaussianMixture(n_components=2, covariance_type='full', verbose=3).fit(X_train_C2)\n",
    "GMM_C3 = mixture.GaussianMixture(n_components=2, covariance_type='full', verbose=3).fit(X_train_C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7b24263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the GMM models\n",
    "preds_0 = GMM_C0.predict(X_test_C0)\n",
    "preds_1 = GMM_C1.predict(X_test_C1)\n",
    "preds_2 = GMM_C2.predict(X_test_C2)\n",
    "preds_3 = GMM_C3.predict(X_test_C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec9d2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data using GMM and save the data\n",
    "\n",
    "data_0, prediction_0 = GMM_C0.sample(10000)\n",
    "data_1, prediction_1 = GMM_C0.sample(10000)\n",
    "data_2, prediction_2 = GMM_C0.sample(10000)\n",
    "data_3, prediction_3 = GMM_C0.sample(10000)\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C0_generelized.csv\", data_0, delimiter=\",\")\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C1_generelized.csv\", data_1, delimiter=\",\")\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C2_generelized.csv\", data_2, delimiter=\",\")\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C3_generelized.csv\", data_3, delimiter=\",\")\n",
    "\n",
    "\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C0_generelized_labels.csv\", prediction_0, delimiter=\",\")\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C1_generelized_labels.csv\", prediction_1, delimiter=\",\")\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C2_generelized_labels.csv\", prediction_2, delimiter=\",\")\n",
    "np.savetxt(\"D:/HOLDA-main/activity/GMM_data/synthatic_data_C3_generelized_labels.csv\", prediction_3, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c96d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5677651515151515\n",
      "0.6102651515151515\n",
      "0.07007575757575757\n",
      "0.9253787878787879\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy of the GMM models\n",
    "acc_0=accuracy_score(y_test_C0, preds_0)\n",
    "acc_1=accuracy_score(y_test_C1, preds_1)\n",
    "acc_2=accuracy_score(y_test_C2, preds_2)\n",
    "acc_3=accuracy_score(y_test_C3, preds_3)\n",
    "print(acc_0)\n",
    "print(acc_1)\n",
    "print(acc_2)\n",
    "print(acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a29e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to extract the rules\n",
    "\n",
    "def get_rule(x, y, dt, feature_names, class_name, numeric_columns, encdec = None, multi_label=False, constraints = None):\n",
    "    x = x.reshape(1, -1)\n",
    "    feature = dt.tree_.feature\n",
    "#     print('feature: ', feature)\n",
    "    threshold = dt.tree_.threshold\n",
    "    label  = dt.predict(x)\n",
    "    leave_id = dt.apply(x)\n",
    "    node_index = dt.decision_path(x).indices\n",
    "    premises = list()\n",
    "    for node_id in node_index:\n",
    "        rule = dict()\n",
    "        if leave_id[0] == node_id:\n",
    "            for p in premises:\n",
    "                if p[1] == '>':\n",
    "                    rule[p[0]]= [p[2], np.Infinity]\n",
    "#                     print(att)\n",
    "#                     print(\"1111111111\")\n",
    "                else:\n",
    "                    rule[p[0]]= [-np.Infinity, p[2]]\n",
    "#                     print(att)\n",
    "#                     print(\"22222222222\")\n",
    "            rule['label'] = int(label.item())\n",
    "#             print(rule)\n",
    "            return (rule, node_id)\n",
    "#             with open(\"sample.json\", \"a\") as outfile:\n",
    "#                 json.dump(rule, outfile)\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            att = feature_names[feature[node_id]]\n",
    "            if att not in numeric_columns:\n",
    "#                 print('categorical column')\n",
    "                # caso di variabile categorica\n",
    "                #op = '<' if x[0][feature[node_id]] <= threshold[node_id] else '>'\n",
    "                #ind = feature_names[feature[node_id]]\n",
    "                thr = 0 if threshold[node_id] <= 0 else 1\n",
    "                op = '='\n",
    "            else:\n",
    "#                 print('numeric column')\n",
    "                op = '<=' if x[0][feature[node_id]] <= threshold[node_id] else '>'\n",
    "                att = [feature[node_id]]\n",
    "#                 att = feature_names[feature[node_id]]\n",
    "#                 print('att', att)\n",
    "#                 print('feature[node_id]', feature[node_id])\n",
    "                thr = threshold[node_id]\n",
    "        premises.append((str(att[0]), op, thr))\n",
    "\n",
    "#     dt_outcome = dt.predict(x)[0]\n",
    "#     cons = class_values[int(dt_outcome)] if not multi_label else multilabel2str(dt_outcome, class_values)\n",
    "#     premises = compact_premises(premises)\n",
    "#     return Rule(premises, cons, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "990ef222",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = list()\n",
    "for i in range(len(names)):\n",
    "    numeric_columns.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51b52ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name\n",
    "merged_final_rule = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb64f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73076494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "# extract teh rules from tree 0\n",
    "rule_total = dict()\n",
    "ids = list()\n",
    "final_rule = list()\n",
    "counter = 0\n",
    "for i in range(len(X_train_C0)):\n",
    "    rule_temp, nd_id =get_rule(X_train_C0[i], y_train_C0[i], tree_C0, names, class_name, names, encdec = None, multi_label=False, constraints = None)\n",
    "    if nd_id not in rule_total.keys():\n",
    "        rule_total[nd_id] = dict()\n",
    "        rule_total[nd_id]['repetition'] = 1\n",
    "        rule_total[nd_id]['rule'] = rule_temp\n",
    "    else:\n",
    "        rule_total[nd_id]['repetition'] +=1\n",
    "        \n",
    "for node in rule_total:\n",
    "    if  rule_total[node]['repetition'] > k:\n",
    "        counter+=1\n",
    "        final_rule.append(rule_total[node]['rule'])\n",
    "# rule_total.append(rule_temp)\n",
    "# ids.append(nd_id)\n",
    "with open(\"Rule_tree_0_generelized.json\", \"w\") as outfile:\n",
    "    json.dump(final_rule, outfile)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01299016",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_rule)):\n",
    "    merged_final_rule.append(final_rule[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract teh rules from tree 1\n",
    "rule_total = dict()\n",
    "ids = list()\n",
    "final_rule = list()\n",
    "counter = 0\n",
    "for i in range(len(X_train_C1)):\n",
    "    rule_temp, nd_id =get_rule(X_train_C1[i], y_train_C1[i], tree_C1, names, class_name, names, encdec = None, multi_label=False, constraints = None)\n",
    "    if nd_id not in rule_total.keys():\n",
    "        rule_total[nd_id] = dict()\n",
    "        rule_total[nd_id]['repetition'] = 1\n",
    "        rule_total[nd_id]['rule'] = rule_temp\n",
    "    else:\n",
    "        rule_total[nd_id]['repetition'] +=1\n",
    "        \n",
    "for node in rule_total:\n",
    "    if  rule_total[node]['repetition'] > k:\n",
    "        counter+=1\n",
    "        final_rule.append(rule_total[node]['rule'])\n",
    "# rule_total.append(rule_temp)\n",
    "# ids.append(nd_id)\n",
    "with open(\"Rule_tree_1_generelized.json\", \"w\") as outfile:\n",
    "    json.dump(final_rule, outfile)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_rule)):\n",
    "    merged_final_rule.append(final_rule[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract teh rules from tree 2\n",
    "rule_total = dict()\n",
    "ids = list()\n",
    "final_rule = list()\n",
    "counter = 0\n",
    "for i in range(len(X_train_C2)):\n",
    "    rule_temp, nd_id =get_rule(X_train_C2[i], y_train_C2[i], tree_C2, names, class_name, names, encdec = None, multi_label=False, constraints = None)\n",
    "    if nd_id not in rule_total.keys():\n",
    "        rule_total[nd_id] = dict()\n",
    "        rule_total[nd_id]['repetition'] = 1\n",
    "        rule_total[nd_id]['rule'] = rule_temp\n",
    "    else:\n",
    "        rule_total[nd_id]['repetition'] +=1\n",
    "        \n",
    "for node in rule_total:\n",
    "    if  rule_total[node]['repetition'] > k:\n",
    "        counter+=1\n",
    "        final_rule.append(rule_total[node]['rule'])\n",
    "# rule_total.append(rule_temp)\n",
    "# ids.append(nd_id)\n",
    "with open(\"Rule_tree_2_generelized.json\", \"w\") as outfile:\n",
    "    json.dump(final_rule, outfile)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a980b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_rule)):\n",
    "    merged_final_rule.append(final_rule[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract teh rules from tree 2\n",
    "rule_total = dict()\n",
    "ids = list()\n",
    "final_rule = list()\n",
    "counter = 0\n",
    "for i in range(len(X_train_C3)):\n",
    "    rule_temp, nd_id =get_rule(X_train_C3[i], y_train_C3[i], tree_C3, names, class_name, names, encdec = None, multi_label=False, constraints = None)\n",
    "    if nd_id not in rule_total.keys():\n",
    "        rule_total[nd_id] = dict()\n",
    "        rule_total[nd_id]['repetition'] = 1\n",
    "        rule_total[nd_id]['rule'] = rule_temp\n",
    "    else:\n",
    "        rule_total[nd_id]['repetition'] +=1\n",
    "        \n",
    "for node in rule_total:\n",
    "    if  rule_total[node]['repetition'] > k:\n",
    "        counter+=1\n",
    "        final_rule.append(rule_total[node]['rule'])\n",
    "# rule_total.append(rule_temp)\n",
    "# ids.append(nd_id)\n",
    "with open(\"Rule_tree_3_generelized.json\", \"w\") as outfile:\n",
    "    json.dump(final_rule, outfile)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76b6ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_rule)):\n",
    "    merged_final_rule.append(final_rule[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6375a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Rule_tree_merged_generelized_non_iid.json\", \"w\") as outfile:\n",
    "    json.dump(merged_final_rule, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
